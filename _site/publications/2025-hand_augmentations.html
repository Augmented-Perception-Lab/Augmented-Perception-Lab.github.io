<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="application-name" content="CMU Augmented Perception Lab" />
  <meta name="theme-color" content="#b00" />
  
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap"
    rel="stylesheet"
  />
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&display=swap" rel="stylesheet">

  <link
    href="https://use.fontawesome.com/releases/v5.13.0/css/all.css"
    rel="stylesheet"
  />
  <link href="/styles.css" rel="stylesheet" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="shortcut icon" href="/favicon.ico" />
  <link
    rel="icon"
    type="image/png"
    href="/assets/logo-sphere-03-01.png"
    sizes="250x250"
  />

  <link
    rel="alternate"
    type="application/rss+xml"
    title="CMU Augmented Perception Lab"
    href="http://localhost:4000/feed.xml"
  />

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Exploring AR hand augmentations as error feedback mechanisms for enhancing gesture-based tutorials | CMU Augmented Perception Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Exploring AR hand augmentations as error feedback mechanisms for enhancing gesture-based tutorials" />
<meta name="author" content="Catarina G. Fidalgo" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Self-guided tutorials from videos help users learn new skills and complete tasks with varying complexity, from repairing a gadget to learning how to play an instrument. However, users may struggle to interpret 3D movements and gestures from 2D representations due to different viewpoints, occlusions, and depth perception. Augmented Reality (AR) can alleviate this challenge by enabling users to view complex instructions in their 3D space. However, most approaches only provide feedback if a live expert is present and do not consider self-guided tutorials. Our work explores virtual hand augmentations as automatic feedback mechanisms to enhance self-guided, gesture-based AR tutorials. We evaluated different error feedback designs and hand placement strategies on speed, accuracy and preference in a user study with 18 participants. Specifically, we investigate two visual feedback styles — color feedback, which changes the color of the hands’ joints to signal pose correctness, and shape feedback, which exaggerates fingers length to guide correction — as well as two placement strategies: superimposed, where the feedback hand overlaps the user’s own, and adjacent, where it appears beside the user’s hand. Results show significantly faster replication time when users are provided with color or baseline no explicit feedback, when compared to shape manipulation feedback. Furthermore, despite users’ preferences for adjacent placement for the feedback representation, superimposed placement significantly reduces replication time. We found no effects on accuracy for short-time recall, suggesting that while these factors may influence task efficiency, they may not strongly affect overall task proficiency." />
<meta property="og:description" content="Self-guided tutorials from videos help users learn new skills and complete tasks with varying complexity, from repairing a gadget to learning how to play an instrument. However, users may struggle to interpret 3D movements and gestures from 2D representations due to different viewpoints, occlusions, and depth perception. Augmented Reality (AR) can alleviate this challenge by enabling users to view complex instructions in their 3D space. However, most approaches only provide feedback if a live expert is present and do not consider self-guided tutorials. Our work explores virtual hand augmentations as automatic feedback mechanisms to enhance self-guided, gesture-based AR tutorials. We evaluated different error feedback designs and hand placement strategies on speed, accuracy and preference in a user study with 18 participants. Specifically, we investigate two visual feedback styles — color feedback, which changes the color of the hands’ joints to signal pose correctness, and shape feedback, which exaggerates fingers length to guide correction — as well as two placement strategies: superimposed, where the feedback hand overlaps the user’s own, and adjacent, where it appears beside the user’s hand. Results show significantly faster replication time when users are provided with color or baseline no explicit feedback, when compared to shape manipulation feedback. Furthermore, despite users’ preferences for adjacent placement for the feedback representation, superimposed placement significantly reduces replication time. We found no effects on accuracy for short-time recall, suggesting that while these factors may influence task efficiency, they may not strongly affect overall task proficiency." />
<link rel="canonical" href="http://localhost:4000/publications/2025-hand_augmentations.html" />
<meta property="og:url" content="http://localhost:4000/publications/2025-hand_augmentations.html" />
<meta property="og:site_name" content="CMU Augmented Perception Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-04T17:49:31-04:00" />
<script type="application/ld+json">
{"headline":"Exploring AR hand augmentations as error feedback mechanisms for enhancing gesture-based tutorials","dateModified":"2025-09-04T17:49:31-04:00","datePublished":"2025-09-04T17:49:31-04:00","@type":"BlogPosting","url":"http://localhost:4000/publications/2025-hand_augmentations.html","author":{"@type":"Person","name":"Catarina G. Fidalgo"},"description":"Self-guided tutorials from videos help users learn new skills and complete tasks with varying complexity, from repairing a gadget to learning how to play an instrument. However, users may struggle to interpret 3D movements and gestures from 2D representations due to different viewpoints, occlusions, and depth perception. Augmented Reality (AR) can alleviate this challenge by enabling users to view complex instructions in their 3D space. However, most approaches only provide feedback if a live expert is present and do not consider self-guided tutorials. Our work explores virtual hand augmentations as automatic feedback mechanisms to enhance self-guided, gesture-based AR tutorials. We evaluated different error feedback designs and hand placement strategies on speed, accuracy and preference in a user study with 18 participants. Specifically, we investigate two visual feedback styles — color feedback, which changes the color of the hands’ joints to signal pose correctness, and shape feedback, which exaggerates fingers length to guide correction — as well as two placement strategies: superimposed, where the feedback hand overlaps the user’s own, and adjacent, where it appears beside the user’s hand. Results show significantly faster replication time when users are provided with color or baseline no explicit feedback, when compared to shape manipulation feedback. Furthermore, despite users’ preferences for adjacent placement for the feedback representation, superimposed placement significantly reduces replication time. We found no effects on accuracy for short-time recall, suggesting that while these factors may influence task efficiency, they may not strongly affect overall task proficiency.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/publications/2025-hand_augmentations.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="content">
      <header class="bg-white">
        <div class="w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l">
        <!-- <div class="headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l"> -->
        <!-- <div class="headermenu w-100 mw8 pa2 flex flex-column">           -->
          <a href="/" class="dib f2 mt4 fw6 link black hover-cmu-red">
          <!-- <a href="/" class="dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2"> -->
            <!-- <picture> -->
              <!-- <source  
                srcset="/assets/logo-sphere-01-01-01.svg"
                type="image/svg+xml"              
              /> -->
              <!-- <source
                srcset="/assets/logo-sphere-03-01.webp"
                type="image/webp"
              /> -->
              <!-- <source  
                srcset="/assets/logo-sphere-03-01.png"
                type="image/png"
              />
              <img 
                class="logo"
                alt="A coarsely tesselated sphere colored in shades of gray."
                class="pl2 w3"
              /> -->              
            <!-- </picture> -->
              Augmented Perception Lab
          </a>
          <!--  --><nav class="mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns">
            <!-- <nav class="mt2 lh-copy w-100 w-25 pa3 mr2"></nav> -->
            <div class="tc mt0-ns mt2">
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/index">Home</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/team">Team</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/publications">Publications</a>
              <!-- <a
                class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  "
                href="/teaching"
                >Teaching</a
              > -->
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/contact">Contact</a>
            </div>
          </nav>
        </div>

      </header>

      <main>
        <section>
  <div class="pv4 bg-white">
    <div class="w-100 mw8 ph4-l ph3 pv2-l pv3 center">
      <h1 class="f2 lh-title measure mt3">
        Exploring AR hand augmentations as error feedback mechanisms for enhancing gesture-based tutorials
      </h1>

      

      <div class="mb2">
        
          Catarina G. Fidalgo,
          Yukang Yan,
          Mouricio Sousa,
          Joaquim Jorge,
          David Lindlbauer.
        <!-- . -->
        </div>

      <!-- <div class="flex flex-row flex-wrap items-start mt1">
       
        <div class="flex flex-column items-center mt3 mr4">
          <picture>
            <source srcset=""></source>
            <source srcset="/assets/person.png"></source>
            <img class="br-100 w3 h3 mb1" alt="Picture of Catarina G. Fidalgo" />
          </picture>
          <div class="black mw4 tc">Catarina G. Fidalgo</div>
        </div>
      
      </div> -->

      
        <div class="mt3">
          Published at
          <span class="b">
            
              <a href="https://www.frontiersin.org/journals/virtual-reality/" class="black underline-dot hover-cmu-red link">
            
            Frontiers in Virtual Reality
            2025
            </a>
          </span>
        </div>
      

      
    </div>
  </div>

  <div class="w-100 mw8 ph4-l ph3 center mt1">
    
    <img src="/assets/publications/2025-hand_augmentations.png" alt="Teaser image" class="mw-100" style="max-height: 600px">

    

    
    
      <h2>Abstract</h2>
      <div class="lh-copy">
        Self-guided tutorials from videos help users learn new skills and complete tasks with varying complexity, from repairing a gadget to learning how to play an instrument. However, users may struggle to interpret 3D movements and gestures from 2D representations due to different viewpoints, occlusions, and depth perception. Augmented Reality (AR) can alleviate this challenge by enabling users to view complex instructions in their 3D space. However, most approaches only provide feedback if a live expert is present and do not consider self-guided tutorials. Our work explores virtual hand augmentations as automatic feedback mechanisms to enhance self-guided, gesture-based AR tutorials. We evaluated different error feedback designs and hand placement strategies on speed, accuracy and preference in a user study with 18 participants. Specifically, we investigate two visual feedback styles — color feedback, which changes the color of the hands’ joints to signal pose correctness, and shape feedback, which exaggerates fingers length to guide correction — as well as two placement strategies: superimposed, where the feedback hand overlaps the user’s own, and adjacent, where it appears beside the user’s hand. Results show significantly faster replication time when users are provided with color or baseline no explicit feedback, when compared to shape manipulation feedback. Furthermore, despite users’ preferences for adjacent placement for the feedback representation, superimposed placement significantly reduces replication time. We found no effects on accuracy for short-time recall, suggesting that while these factors may influence task efficiency, they may not strongly affect overall task proficiency.
      </div>
    

    <!-- width="400" height="240" class="thumb-video"  -->
    

    
    <h2>Materials</h2>
      <ul class="list pl0">
        
          <li class="mt2"><a href="https://doi.org/10.3389/frvir.2025.1574965" class="black link hover-cmu-red underline-dot ">
            PDF
          </a></li>
        

        

        

        

        

        

        

        

        
        
      </ul>
    

    
      <h2>Bibtex</h2>
      <div class="f6 underline-dot">
        <pre>@article {Fidalgo2025-dh, 
 author = {Fidalgo, Catarina G and Yan, Yukang and Sousa, Mauricio and Jorge, Joaquim and Lindlbauer, David}, 
 title = {Exploring {AR} hand augmentations as error feedback mechanisms for enhancing gesture-based tutorials}, 
 year = {2025}, 
 volume = {6}, 
 journal = {Frontiers in virtual reality}, 
 address = {New York, NY, USA}, 
 keywords = {ugmented reality; training; tutorials; hand gestures; error feedback; virtual hand augmentations}, 
 doi = {10.3389/frvir.2025.1574965}, 
 series = {CHI '25}
 }</pre>
      </div>
    
    
  </div>
</section>

      </main>
    </div>

    <footer class="bt white mt5 flex-shrink-0">
      <div class="w-100 mw8 ph4-l ph3 center pv3 footerinfo">
        <ul class="list pl0">
          <li>
            <a href="/index" class="link white dib underline-dot-white hover-cmu-red mv1">Augmented Perception Lab</a>, 
            <a href="https://hcii.cmu.edu" class="link white underline-dot-white hover-cmu-red mv1">Human-Computer Interaction Institute</a>,
            <a href="https://www.cs.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">School of Computer Science</a>,
            <a href="https://www.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">Carnegie Mellon University</a>
            <!-- <span ></span>  -->
          </li>
          <li>
            <span>
            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, <a href="/contact" class="link white dim underline-dot-white hover-cmu-red">How to find us.</a>
            </span>
          </li>
          <li class="pv1">
            <abbr title="Last build on 2025-09-04" class="white" style="list-style: height 2em; text-decoration: none;">Last update September 2025</abbr>              
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
