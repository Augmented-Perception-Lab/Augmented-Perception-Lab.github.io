<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="application-name" content="CMU Augmented Perception Lab" />
  <meta name="theme-color" content="#b00" />
  
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap"
    rel="stylesheet"
  />
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&display=swap" rel="stylesheet">

  <link
    href="https://use.fontawesome.com/releases/v5.13.0/css/all.css"
    rel="stylesheet"
  />
  <link href="/styles.css" rel="stylesheet" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="shortcut icon" href="/favicon.ico" />
  <link
    rel="icon"
    type="image/png"
    href="/assets/logo-sphere-03-01.png"
    sizes="250x250"
  />

  <link
    rel="alternate"
    type="application/rss+xml"
    title="CMU Augmented Perception Lab"
    href="http://localhost:4000/feed.xml"
  />

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>User Preference for Navigation Instructions in Mixed Reality | CMU Augmented Perception Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="User Preference for Navigation Instructions in Mixed Reality" />
<meta name="author" content="Jaewook Lee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users’ visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users’ contexts." />
<meta property="og:description" content="Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users’ visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users’ contexts." />
<link rel="canonical" href="http://localhost:4000/publications/2022-MRNavigation.html" />
<meta property="og:url" content="http://localhost:4000/publications/2022-MRNavigation.html" />
<meta property="og:site_name" content="CMU Augmented Perception Lab" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-07-08T09:36:24-04:00" />
<script type="application/ld+json">
{"description":"Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users’ visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users’ contexts.","@type":"BlogPosting","headline":"User Preference for Navigation Instructions in Mixed Reality","dateModified":"2024-07-08T09:36:24-04:00","datePublished":"2024-07-08T09:36:24-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/publications/2022-MRNavigation.html"},"author":{"@type":"Person","name":"Jaewook Lee"},"url":"http://localhost:4000/publications/2022-MRNavigation.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="content">
      <header class="bg-white">
        <div class="w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l">
        <!-- <div class="headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l"> -->
        <!-- <div class="headermenu w-100 mw8 pa2 flex flex-column">           -->
          <a href="/" class="dib f2 mt4 fw6 link black hover-cmu-red">
          <!-- <a href="/" class="dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2"> -->
            <!-- <picture> -->
              <!-- <source  
                srcset="/assets/logo-sphere-01-01-01.svg"
                type="image/svg+xml"              
              /> -->
              <!-- <source
                srcset="/assets/logo-sphere-03-01.webp"
                type="image/webp"
              /> -->
              <!-- <source  
                srcset="/assets/logo-sphere-03-01.png"
                type="image/png"
              />
              <img 
                class="logo"
                alt="A coarsely tesselated sphere colored in shades of gray."
                class="pl2 w3"
              /> -->              
            <!-- </picture> -->
              Augmented Perception Lab
          </a>
          <!--  --><nav class="mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns">
            <!-- <nav class="mt2 lh-copy w-100 w-25 pa3 mr2"></nav> -->
            <div class="tc mt0-ns mt2">
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/index">Home</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/team">Team</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/publications">Publications</a>
              <!-- <a
                class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  "
                href="/teaching"
                >Teaching</a
              > -->
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/contact">Contact</a>
            </div>
          </nav>
        </div>

      </header>

      <main>
        <section>
  <div class="pv4 bg-white">
    <div class="w-100 mw8 ph4-l ph3 pv2-l pv3 center">
      <h1 class="f2 lh-title measure mt3">
        User Preference for Navigation Instructions in Mixed Reality
      </h1>

      

      <div class="mb2">
        
          Jaewook Lee,
          Fanjie Jin,
          Younsoo Kim,
          David Lindlbauer.
        <!-- . -->
        </div>

      <!-- <div class="flex flex-row flex-wrap items-start mt1">
       
        <div class="flex flex-column items-center mt3 mr4">
          <picture>
            <source srcset=""></source>
            <source srcset="/assets/person.png"></source>
            <img class="br-100 w3 h3 mb1" alt="Picture of Jaewook Lee" />
          </picture>
          <div class="black mw4 tc">Jaewook Lee</div>
        </div>
      
      </div> -->

      
        <div class="mt3">
          Published at
          <span class="b">
            
              <a href="https://ieeevr.org/2022/" class="black underline-dot hover-cmu-red link">
            
            IEEE VR
            2022
            </a>
          </span>
        </div>
      

      
    </div>
  </div>

  <div class="w-100 mw8 ph4-l ph3 center mt1">
    
    <img src="/assets/publications/2022-MRNavigation.png" alt="Teaser image" class="mw-100" style="max-height: 600px">

    

    
    
      <h2>Abstract</h2>
      <div class="lh-copy">
        Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions.
Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users' visual field, potentially making them less obtrusive and more expressive.
Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities.
While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users.
We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task.
In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios.
Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives.
We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study.
Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users' contexts.

      </div>
    

    <!-- width="400" height="240" class="thumb-video"  -->
    
    <iframe width="600" height="360" class="thumb-video mt3 mb2" src="https://www.youtube.com/embed/4E5Ca-bVsxU?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen>
    </iframe>   

    <!-- <li class="mt2"><a href="https://www.youtube.com/watch?v=4E5Ca-bVsxU" class="black link hover-cmu-red underline-dot ">
      Video: 30sec teaser
    </a></li> -->
    

    
    <h2>Materials</h2>
      <ul class="list pl0">
        
          <li class="mt2"><a href="https://ieeexplore.ieee.org/document/9756757/" class="black link hover-cmu-red underline-dot ">
            PDF
          </a></li>
        

        

        

        
          <li class="mt2"><a href="https://www.youtube.com/watch?v=4E5Ca-bVsxU" class="black link hover-cmu-red underline-dot ">
            Video: 30sec teaser
          </a></li>
        

        
          <li class="mt2"><a href="https://www.youtube.com/watch?v=vRyM7TEQHWM" class="black link hover-cmu-red underline-dot ">
            Video: Full length video
          </a></li>
        

        
          <li class="mt2"><a href="https://www.youtube.com/watch?v=UvP8FJ9Q0x8" class="black link hover-cmu-red underline-dot ">
            Video: Short presentation
          </a></li>
        

        

        

        
        
      </ul>
    

    
      <h2>Bibtex</h2>
      <div class="f6 underline-dot">
        <pre>@inproceedings {Lee2022, 
 author = {Lee, Jaewook and Jin, Fanjie and Kim, Younsoo and Lindlbauer, David}, 
 title = {User Preference for Navigation Instructions in Mixed Reality}, 
 year = {2022}, 
 publisher = {IEEE}, 
 keywords = {Mixed Reality, Navigation, Adaptive user interfaces}, 
 location = {Virtual Event, New Zealand}, 
 series = {IEEE VR '2022} 
 }</pre>
      </div>
    
    
  </div>
</section>

      </main>
    </div>

    <footer class="bt white mt5 flex-shrink-0">
      <div class="w-100 mw8 ph4-l ph3 center pv3 footerinfo">
        <ul class="list pl0">
          <li>
            <a href="/index" class="link white dib underline-dot-white hover-cmu-red mv1">Augmented Perception Lab</a>, 
            <a href="https://hcii.cmu.edu" class="link white underline-dot-white hover-cmu-red mv1">Human-Computer Interaction Institute</a>,
            <a href="https://www.cs.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">School of Computer Science</a>,
            <a href="https://www.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">Carnegie Mellon University</a>
            <!-- <span ></span>  -->
          </li>
          <li>
            <span>
            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, <a href="/contact" class="link white dim underline-dot-white hover-cmu-red">How to find us.</a>
            </span>
          </li>
          <li class="pv1">
            <abbr title="Last build on 2024-07-08" class="white" style="list-style: height 2em; text-decoration: none;">Last update July 2024</abbr>              
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
