<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="x-ua-compatible" content="ie=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="application-name" content="CMU Augmented Perception Lab" />
  <meta name="theme-color" content="#b00" />
  
  <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin />
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&display=swap"
    rel="stylesheet"
  />
  
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&display=swap" rel="stylesheet">

  <link
    href="https://use.fontawesome.com/releases/v5.13.0/css/all.css"
    rel="stylesheet"
  />
  <link href="/styles.css" rel="stylesheet" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <link rel="shortcut icon" href="/favicon.ico" />
  <link
    rel="icon"
    type="image/png"
    href="/assets/logo-light.png"
    sizes="250x250"
  />

  <link
    rel="alternate"
    type="application/rss+xml"
    title="CMU Augmented Perception Lab"
    href="http://localhost:4000/feed.xml"
  />

  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Publications | CMU Augmented Perception Lab</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Augmented Perception Lab at Carnegie Mellon University in Pittsburgh." />
<meta property="og:description" content="Augmented Perception Lab at Carnegie Mellon University in Pittsburgh." />
<link rel="canonical" href="http://localhost:4000/publications.html" />
<meta property="og:url" content="http://localhost:4000/publications.html" />
<meta property="og:site_name" content="CMU Augmented Perception Lab" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"description":"Augmented Perception Lab at Carnegie Mellon University in Pittsburgh.","headline":"Publications","url":"http://localhost:4000/publications.html","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <div class="content">
      <header class="bg-white">
        <div class="w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l">
        <!-- <div class="headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l"> -->
        <!-- <div class="headermenu w-100 mw8 pa2 flex flex-column">           -->
          <a href="/" class="dib f2 mt4 fw6 link black hover-cmu-red">
          <!-- <a href="/" class="dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2"> -->
            <!-- <picture> -->
              <!-- <source  
                srcset="/assets/logo-sphere-01-01-01.svg"
                type="image/svg+xml"              
              /> -->
              <!-- <source
                srcset="/assets/logo-light.webp"
                type="image/webp"
              /> -->
              <!-- <source  
                srcset="/assets/logo-sphere-03-01.png"
                type="image/png"
              />
              <img 
                class="logo"
                alt="A coarsely tesselated sphere colored in shades of gray."
                class="pl2 w3"
              /> -->              
            <!-- </picture> -->
              Augmented Perception Lab
          </a>
          <!--  --><nav class="mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns">
            <!-- <nav class="mt2 lh-copy w-100 w-25 pa3 mr2"></nav> -->
            <div class="tc mt0-ns mt2">
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/index">Home</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/team">Team</a>
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib cmu-red " href="/publications">Publications</a>
              <!-- <a
                class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  "
                href="/teaching"
                >Teaching</a
              > -->
              <a class="ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  " href="/contact">Contact</a>
            </div>
          </nav>
        </div>

      </header>

      <main>
        <section>
  <!-- <div class="pv4 bg-near-white">
    <div class="w-100 mw8 ph4-l ph3 pv4-l pv3 center">
      <h1 class="f2 lh-title measure mt3">
        Publications
      </h1>
      <p class="f4 measure lh-copy">We publish at premier venues in Human-Computer Interaction, such as ACM CHI or ACM UIST.</p>
    </div>
  </div> -->

  <div class="w-100 mw8 ph4-l ph3 center mt4">
    <!-- <div id="facets" class="dn flex flex-row flex-wrap">
  <div class="facet mr3" id="venue_tags">
    <strong>Venue</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="authors">
    <strong>Author</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="tags">
    <strong>Tag</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="type">
    <strong>Type</strong>
    <ul class="list pl0"></ul>
  </div>
  <div class="facet mr3" id="awards">
    <strong>Award</strong>
    <ul class="list pl0"></ul>
  </div>

</div>

<label id="only-highlight" class="dn">
  <input type="checkbox" id="highlight">
  Show only highlights
</label>

<p id="clear-filters" class="dn b pointer">
  <i class="fas fa-times-circle cmu-red" aria-hidden="true"></i> Clear all filters. <span id="count_hidden">X</span> of
  <span id="count_total">X</span> publications are hidden by the filters.
</p> -->

  <!-- <p >
    We publish at premier venues in Human-Computer Interaction, such as ACM CHI or ACM UIST.
  </p> -->


<article>
   
   
  
  
    <h2 class="year f3" id="y2020">2020</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;excerpt&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;excerpt&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;},\&quot;headline\&quot;:\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Christian Holz,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-omni.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/N9RPUSfnBac?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415589\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/omni/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XatgUCTpCWQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ISR7eQgD5x8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=q7PoZ-jGN3I\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;},\&quot;headline\&quot;:\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Christian Holz,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-omni.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/N9RPUSfnBac?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415589\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/omni/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XatgUCTpCWQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ISR7eQgD5x8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=q7PoZ-jGN3I\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-omni_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/N9RPUSfnBac?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-omni">
                  <a href="/publications/2020-omni.html" class="black hover-cmu-red link">Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback</a>
</h3>
              
                  <div class="mb2">
                
                  Thomas Langerak, 
                  Juan José Zárate, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Christian Holz, 
                  Otmar Hilliges.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2020
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2020-omni.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3379337.3415589" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3379337.3415589" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://ait.ethz.ch/projects/2020/omni/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="XatgUCTpCWQ" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_omni-volumetric-sensing-and-actuation-of-passive-magnetic-tools-for-dynamic-haptic-feedback" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_omni-volumetric-sensing-and-actuation-of-passive-magnetic-tools-for-dynamic-haptic-feedback">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings { Langerack20a, 
 author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, 
 title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, 
 year = {2020}, 
 isbn = {9781450375146}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3379337.3415593}, 
 doi = {10.1145/3379337.3415593}, 
 pages = {951–965}, 
 numpages = {15}, 
 keywords = {optimal control, haptic devices, computational interaction}, 
 location = {Virtual Event, USA}, 
 series = {UIST '20} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;excerpt&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;},\&quot;headline\&quot;:\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Christian Holz,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-omni.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/N9RPUSfnBac?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415589\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/omni/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XatgUCTpCWQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ISR7eQgD5x8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=q7PoZ-jGN3I\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-magpen_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-magpen">
                  <a href="/publications/2020-magpen.html" class="black hover-cmu-red link">Optimal Control for Electromagnetic Haptic Guidance Systems</a>
</h3>
              
                  <div class="mb2">
                
                  Thomas Langerak, 
                  Juan José Zárate, 
                  Velko Vechev, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Daniele Panozzo, 
                  Otmar Hilliges.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2020
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2020-magpen.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3379337.3415593" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3379337.3415589" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://ait.ethz.ch/projects/2020/magpen/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="XETEOOj0QI8" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_optimal-control-for-electromagnetic-haptic-guidance-systems" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_optimal-control-for-electromagnetic-haptic-guidance-systems">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Langerak20b, 
 author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, 
 title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, 
 year = {2020}, 
 isbn = {9781450375146}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, 
 pages = {951–965}, 
 numpages = {15}, 
 keywords = {haptic devices, optimal control, computational interaction}, 
 location = {Virtual Event, USA}, 
 series = {UIST '20} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:null,&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;excerpt&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-omni.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni’s 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni’s hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;},\&quot;headline\&quot;:\&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-omni.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Christian Holz,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-omni.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Omni, a self-contained 3D haptic feedback system that is capable of sensing and actuating an untethered, passive tool containing only a small embedded permanent magnet. Omni enriches AR, VR and desktop applications by providing an active haptic experience using a simple apparatus centered around an electromagnetic base. The spatial haptic capabilities of Omni are enabled by a novel gradient-based method to reconstruct the 3D position of the permanent magnet in midair using the measurements from eight off-the-shelf hall sensors that are integrated into the base. Omni's 3 DoF spherical electromagnet simultaneously exerts dynamic and precise radial and tangential forces in a volumetric space around the device. Since our system is fully integrated, contains no moving parts and requires no external tracking, it is easy and affordable to fabricate. We describe Omni's hardware implementation, our 3D reconstruction algorithm, and evaluate the tracking and actuation performance in depth. Finally, we demonstrate its capabilities via a set of interactive usage scenarios.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/N9RPUSfnBac?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415589\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/omni/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=N9RPUSfnBac\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XatgUCTpCWQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ISR7eQgD5x8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=q7PoZ-jGN3I\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2020-fatigue_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2020-fatigue">
                  <a href="/publications/2020-fatigue.html" class="black hover-cmu-red link">A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability</a>
</h3>
              
                  <div class="mb2">
                
                  Liliana Barrios, 
                  Pietro Oldrati, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Marc Hilty, 
                  Helen Hayward-Koennecke, 
                  Christian Holz, 
                  Andreas Lutterotti.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2020
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2020-fatigue.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/abs/10.1145/3313831.3376588" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3313831.3376588" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                
                <!-- 
                 -->

                
                <label for="slide_a-rapid-tapping-task-on-commodity-smartphones-to-assess-motor-fatigability" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_a-rapid-tapping-task-on-commodity-smartphones-to-assess-motor-fatigability">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inbook{10.1145/3313831.3376588, 
 author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, 
 title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, 
 year = {2020}, 
 isbn = {9781450367080}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3313831.3376588}, 
 booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, 
 pages = {1–10}, 
 numpages = {10} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2019">2019</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2019-compMR_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2019-compMR">
                  <a href="/publications/2019-compMR.html" class="black hover-cmu-red link">Context-Aware Online Adaptation of Mixed Reality Interfaces</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Anna Maria Feit, 
                  Otmar Hilliges.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2019
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2019-compMR.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3332165.3347945" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3332165.3347945" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://ait.ethz.ch/projects/2019/computationalMR/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="heAGuCsWs9o" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_context-aware-online-adaptation-of-mixed-reality-interfaces" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_context-aware-online-adaptation-of-mixed-reality-interfaces">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings { Lindlbauer2019a, 
 author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, 
 title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},
 year = {2019},
 isbn = {9781450368162},
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3332165.3347945},
 doi = {10.1145/3332165.3347945},
 pages = {147–160},
 numpages = {14},
 keywords = {ui optimization, context-awareness, mixed reality},
 location = {New Orleans, LA, USA},
 series = {UIST '19}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2019-mentalimage_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2019-mentalimage">
                  <a href="/publications/2019-mentalimage.html" class="black hover-cmu-red link">The Mental Image Revealed by Gaze Tracking</a>
</h3>
              
                  <div class="mb2">
                
                  Xi Wang, 
                  Andreas Ley, 
                  Sebastian Koch, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  James Hays, 
                  Kenneth Holmqvist, 
                  Marc Alexa.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2019
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2019-mentalimage.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3290605.3300839" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3290605.3300839" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="a7dBY_EZEUQ" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_the-mental-image-revealed-by-gaze-tracking" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_the-mental-image-revealed-by-gaze-tracking">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Wang19 
 author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, 
 title = {The Mental Image Revealed by Gaze Tracking}, 
 year = {2019}, 
 isbn = {9781450359702}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3290605.3300839}, 
 doi = {10.1145/3290605.3300839}, 
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, 
 pages = {1–12}, 
 numpages = {12}, 
 keywords = {gaze pattern, eye tracking, mental imagery}, 
 location = {Glasgow, Scotland Uk}, 
 series = {CHI '19} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2019-tactiles_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/YVC1BIo-zS4?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2019-tactiles">
                  <a href="/publications/2019-tactiles.html" class="black hover-cmu-red link">TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR</a>
</h3>
              
                  <div class="mb2">
                
                  Velko Vechev, 
                  Juan Zarate, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Ronan Hinchet, 
                  Herbert Shea, 
                  Otmar Hilliges.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                IEEE VR
                2019
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2019-tactiles.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://ieeexplore.ieee.org/document/8797921" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1109/VR.2019.8797921" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://ait.ethz.ch/projects/2019/tactiles/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="YVC1BIo-zS4" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_tactiles-dual-mode-low-power-electromagnetic-actuators-for-rendering-continuous-contact-and-spatial-haptic-patterns-in-vr" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_tactiles-dual-mode-low-power-electromagnetic-actuators-for-rendering-continuous-contact-and-spatial-haptic-patterns-in-vr">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@article{vechev2019, 
 title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, 
 author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, 
 booktitle={2019 IEEE Virtual Reality (VR)}, 
 month = {Mar}, 
 year = {2019}, 
 organization={IEEE},  
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-omni.html&quot;,&quot;id&quot;:&quot;/publications/2020-omni&quot;,&quot;relative_path&quot;:&quot;_publications/2020-omni.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415589&quot;,&quot;title&quot;:&quot;Omni: Volumetric Sensing and Actuation of Passive Magnetic Tools for Dynamic Haptic Feedback&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;David Lindlbauer&quot;,&quot;Christian Holz&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/omni/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-30sec&quot;:&quot;N9RPUSfnBac&quot;,&quot;video-suppl&quot;:&quot;XatgUCTpCWQ&quot;,&quot;video-talk-5min&quot;:&quot;ISR7eQgD5x8&quot;,&quot;video-talk-15min&quot;:&quot;q7PoZ-jGN3I&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Langerack20a, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, \n doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {optimal control, haptic devices, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-omni&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;excerpt&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Optimal Control for Electromagnetic Haptic Guidance Systems | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Thomas Langerak\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-magpen.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;},\&quot;headline\&quot;:\&quot;Optimal Control for Electromagnetic Haptic Guidance Systems\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-magpen.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Thomas Langerak\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Optimal Control for Electromagnetic Haptic Guidance Systems\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Thomas Langerak,\n          Juan José Zárate,\n          Velko Vechev,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Daniele Panozzo,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Thomas Langerak\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Thomas Langerak&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2020/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-magpen.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce an optimal control method for electromagnetic haptic guidance systems. Our real-time approach assists users in pen-based tasks such as drawing, sketching or designing. The key to our control method is that it guides users, yet does not take away agency. Existing approaches force the stylus to a continuously advancing setpoint on a target trajectory, leading to undesirable behavior such as loss of haptic guidance or unintended snapping. Our control approach, in contrast, gently pulls users towards the target trajectory, allowing them to always easily override the system to adapt their input spontaneously and draw at their own speed. To achieve this flexible guidance, our optimization iteratively predicts the motion of an input device such as a pen, and adjusts the position and strength of an underlying dynamic electromagnetic actuator accordingly. To enable real-time computation, we additionally introduce a novel and fast approximate model of an electromagnet. We demonstrate the applicability of our approach by implementing it on a prototypical hardware platform based on an electromagnet moving on a bi-axial linear stage, as well as a set of applications. Experimental results show that our approach is more accurate and preferred by users compared to open-loop and time-dependent closed-loop approaches.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/kWug-YdLwwc?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3379337.3415593\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2020/magpen/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=kWug-YdLwwc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=XETEOOj0QI8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=KJxiGcKHjU0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Short presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NWCfPa9ddds\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-magpen.html&quot;,&quot;id&quot;:&quot;/publications/2020-magpen&quot;,&quot;relative_path&quot;:&quot;_publications/2020-magpen.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3379337.3415593&quot;,&quot;title&quot;:&quot;Optimal Control for Electromagnetic Haptic Guidance Systems&quot;,&quot;authors&quot;:[&quot;Thomas Langerak&quot;,&quot;Juan José Zárate&quot;,&quot;Velko Vechev&quot;,&quot;David Lindlbauer&quot;,&quot;Daniele Panozzo&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2020/magpen/&quot;,&quot;doi&quot;:&quot;10.1145/3379337.3415589&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2020/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Haptics&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-30sec&quot;:&quot;kWug-YdLwwc&quot;,&quot;video-suppl&quot;:&quot;XETEOOj0QI8&quot;,&quot;video-talk-5min&quot;:&quot;KJxiGcKHjU0&quot;,&quot;video-talk-15min&quot;:&quot;NWCfPa9ddds&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Langerak20b, \n author = {Langerak, Thomas and Zarate, Juan Jose and Vechev, Velko and Lindlbauer, David and Panozzo, Daniele and Hilliges, Otmar}, \n title = {Optimal Control for Electromagnetic Haptic Guidance Systems}, \n year = {2020}, \n isbn = {9781450375146}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3379337.3415593}, doi = {10.1145/3379337.3415593}, \n pages = {951–965}, \n numpages = {15}, \n keywords = {haptic devices, optimal control, computational interaction}, \n location = {Virtual Event, USA}, \n series = {UIST '20} \n }&quot;,&quot;slug&quot;:&quot;2020-magpen&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;excerpt&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Liliana Barrios\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients’ every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;},\&quot;headline\&quot;:\&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2020-fatigue.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Liliana Barrios\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Liliana Barrios,\n          Pietro Oldrati,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Marc Hilty,\n          Helen Hayward-Koennecke,\n          Christian Holz,\n          Andreas Lutterotti.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Liliana Barrios\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Liliana Barrios&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2020.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2020\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2020-fatigue.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Fatigue is a common debilitating symptom of many autoimmune diseases, including multiple sclerosis. It negatively impacts patients' every-day life and productivity. Despite its prevalence, fatigue is still poorly understood. Its subjective nature makes quantification challenging and it is mainly assessed by questionnaires, which capture the magnitude of fatigue insufficiently. Motor fatigability, the objective decline of performance during a motor task, is an underrated aspect in this regard. Currently, motor fatigability is assessed using a handgrip dynamometer. This approach has been proven valid and accurate but requires special equipment and trained personnel. We propose a technique to objectively quantify motor fatigability using a commodity smartphone. The method comprises a simple exertion task requiring rapid alternating tapping. Our study with 20 multiple sclerosis patients and 35 healthy participants showed a correlation of rho = 0.8 with the baseline handgrip method. This smartphone-based approach is a first step towards ubiquitous, more frequent, and remote monitoring of fatigability and disease progression.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/zlnX_ZJzD5w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=zlnX_ZJzD5w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=iePm5dlAiio\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2020-fatigue.html&quot;,&quot;id&quot;:&quot;/publications/2020-fatigue&quot;,&quot;relative_path&quot;:&quot;_publications/2020-fatigue.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2020,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/abs/10.1145/3313831.3376588&quot;,&quot;title&quot;:&quot;A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability&quot;,&quot;authors&quot;:[&quot;Liliana Barrios&quot;,&quot;Pietro Oldrati&quot;,&quot;David Lindlbauer&quot;,&quot;Marc Hilty&quot;,&quot;Helen Hayward-Koennecke&quot;,&quot;Christian Holz&quot;,&quot;Andreas Lutterotti&quot;],&quot;doi&quot;:&quot;10.1145/3313831.3376588&quot;,&quot;venue_location&quot;:&quot;Virtual&quot;,&quot;venue_url&quot;:&quot;https://chi2020.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Medical&quot;,&quot;Motor Fatigue&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-30sec&quot;:&quot;zlnX_ZJzD5w&quot;,&quot;video-talk-15min&quot;:&quot;iePm5dlAiio&quot;,&quot;bibtex&quot;:&quot;@inbook{10.1145/3313831.3376588, \n author = {Barrios, Liliana and Oldrati, Pietro and Lindlbauer, David and Hilty, Marc and Hayward-Koennecke, Helen and Holz, Christian and Lutterotti, Andreas}, \n title = {A Rapid Tapping Task on Commodity Smartphones to Assess Motor Fatigability}, \n year = {2020}, \n isbn = {9781450367080}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3313831.3376588}, \n booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–10}, \n numpages = {10} \n }&quot;,&quot;slug&quot;:&quot;2020-fatigue&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;excerpt&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;excerpt&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Velko Vechev\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;},\&quot;headline\&quot;:\&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-tactiles.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Velko Vechev\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Velko Vechev,\n          Juan Zarate,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Ronan Hinchet,\n          Herbert Shea,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Velko Vechev\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Velko Vechev&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ieeevr.org/2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            IEEE VR\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-tactiles.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm 3 ) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ieeexplore.ieee.org/document/8797921\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/tactiles/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=YVC1BIo-zS4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Understanding Metamaterial Mechanisms | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Alexandra Ion\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-umm.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Understanding Metamaterial Mechanisms\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures—known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;},\&quot;headline\&quot;:\&quot;Understanding Metamaterial Mechanisms\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-umm.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Alexandra Ion\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Understanding Metamaterial Mechanisms\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Alexandra Ion,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Patrick Baudisch.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Alexandra Ion\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Alexandra Ion&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-umm.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        In this paper, we establish the underlying foundations of mechanisms that are composed of cell structures---known as metamaterial mechanisms. Such metamaterial mechanisms were previously shown to implement complete mechanisms in the cell structure of a 3D printed material, without the need for assembly. However, their design is highly challenging. A mechanism consists of many cells that are interconnected and impose constraints on each other. This leads to unobvious and non-linear behavior of the mechanism, which impedes user design. In this work, we investigate the underlying topological constraints of such cell structures and their influence on the resulting mechanism. Based on these findings, we contribute a computational design tool that automatically creates a metamaterial mechanism from user-defined motion paths. This tool is only feasible because our novel abstract representation of the global constraints highly reduces the search space of possible cell arrangements.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300877\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8bmEJSOBm_U\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=L6lUH0r-w-o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-umm.html&quot;,&quot;id&quot;:&quot;/publications/2019-umm&quot;,&quot;relative_path&quot;:&quot;_publications/2019-umm.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:1,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300877&quot;,&quot;title&quot;:&quot;Understanding Metamaterial Mechanisms&quot;,&quot;authors&quot;:[&quot;Alexandra Ion&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Patrick Baudisch&quot;],&quot;doi&quot;:&quot;10.1145/3290605.3300877&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Metamaterials&quot;,&quot;Interactive Materials&quot;,&quot;Fabrication&quot;,&quot;Computational design&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-30sec&quot;:&quot;8bmEJSOBm_U&quot;,&quot;video-suppl&quot;:&quot;L6lUH0r-w-o&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Ion19, \n author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, \n title = {Understanding Metamaterial Mechanisms}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300877}, \n doi = {10.1145/3290605.3300877}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–14}, \n numpages = {14}, \n keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-umm&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2019-umm_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/8bmEJSOBm_U?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2019-umm">
                  <a href="/publications/2019-umm.html" class="black hover-cmu-red link">Understanding Metamaterial Mechanisms</a>
</h3>
              
                  <div class="mb2">
                
                  Alexandra Ion, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Philipp Herholz, 
                  Marc Alexa, 
                  Patrick Baudisch.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2019
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2019-umm.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3290605.3300877" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3290605.3300877" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="L6lUH0r-w-o" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_understanding-metamaterial-mechanisms" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_understanding-metamaterial-mechanisms">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Ion19, 
 author = {Ion, Alexandra and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and Baudisch, Patrick}, 
 title = {Understanding Metamaterial Mechanisms}, 
 year = {2019}, 
 isbn = {9781450359702}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3290605.3300877}, 
 doi = {10.1145/3290605.3300877}, 
 booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, 
 pages = {1–14}, 
 numpages = {14}, 
 keywords = {mechanism, microstructure, computational design, fabrication, metamaterials}, 
 location = {Glasgow, Scotland Uk}, 
 series = {CHI '19} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2018">2018</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-tactiles.html&quot;,&quot;id&quot;:&quot;/publications/2019-tactiles&quot;,&quot;relative_path&quot;:&quot;_publications/2019-tactiles.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:3,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;pdf&quot;:&quot;https://ieeexplore.ieee.org/document/8797921&quot;,&quot;title&quot;:&quot;TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR&quot;,&quot;authors&quot;:[&quot;Velko Vechev&quot;,&quot;Juan Zarate&quot;,&quot;David Lindlbauer&quot;,&quot;Ronan Hinchet&quot;,&quot;Herbert Shea&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/tactiles/&quot;,&quot;doi&quot;:&quot;10.1109/VR.2019.8797921&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ieeevr.org/2019/&quot;,&quot;venue_tags&quot;:[&quot;IEEE VR&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Virtual Reality&quot;],&quot;venue&quot;:&quot;IEEE VR&quot;,&quot;video-thumb&quot;:&quot;YVC1BIo-zS4&quot;,&quot;video-suppl&quot;:&quot;YVC1BIo-zS4&quot;,&quot;bibtex&quot;:&quot;@article{vechev2019, \n title={TacTiles: Dual-mode Low-power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR}, \n author={Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar}, \n booktitle={2019 IEEE Virtual Reality (VR)}, \n month = {Mar}, \n year = {2019}, \n organization={IEEE},  \n }&quot;,&quot;slug&quot;:&quot;2019-tactiles&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;excerpt&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;The Mental Image Revealed by Gaze Tracking | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Xi Wang\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;The Mental Image Revealed by Gaze Tracking\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user’s eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;},\&quot;headline\&quot;:\&quot;The Mental Image Revealed by Gaze Tracking\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-mentalimage.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Xi Wang\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        The Mental Image Revealed by Gaze Tracking\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Xi Wang,\n          Andreas Ley,\n          Sebastian Koch,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          James Hays,\n          Kenneth Holmqvist,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Xi Wang\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Xi Wang&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2019.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-mentalimage.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Humans involuntarily move their eyes when retrieving an image from memory. This motion is often similar to actually observing the image. We suggest to exploit this behavior as a new modality in human computer interaction, using the motion of the eyes as a descriptor of the image. Interaction requires the user's eyes to be tracked but no voluntary physical activity. We perform a controlled experiment and develop matching techniques using machine learning to investigate if images can be discriminated based on the gaze patterns recorded while users merely think about image. Our results indicate that image retrieval is possible with an accuracy significantly above chance. We also show that this result generalizes to images not used during training of the classifier and extends to uncontrolled settings in a realistic scenario.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/gaKdA51v5dg?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3290605.3300839\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gaKdA51v5dg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=a7dBY_EZEUQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2018-remixedreality_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2018-remixedreality">
                  <a href="/publications/2018-remixedreality.html" class="black hover-cmu-red link">Remixed Reality: Manipulating Space and Time in Augmented Reality</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Andrew D. Wilson.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2018
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2018-remixedreality.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3173574.3173703" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3173574.3173703" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="GoSQTPfrdCc" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_remixed-reality-manipulating-space-and-time-in-augmented-reality" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_remixed-reality-manipulating-space-and-time-in-augmented-reality">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Lindlbauer18, 
 author = {Lindlbauer, David and Wilson, Andrew D.}, 
 title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, 
 year = {2018}, 
 isbn = {9781450356206}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3173574.3173703}, 
 doi = {10.1145/3173574.3173703}, 
 pages = {1–13}, 
 numpages = {13}, 
 keywords = {remixed reality, virtual reality, augmented reality}, 
 location = {Montreal QC, Canada}, 
 series = {CHI '18} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2017">2017</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2017-heatspace_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2017-heatspace">
                  <a href="/publications/2017-heatspace.html" class="black hover-cmu-red link">HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior</a>
</h3>
              
                  <div class="mb2">
                
                  Andreas Fender, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Philipp Herholz, 
                  Marc Alexa, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2017
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2017-heatspace.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3126594.3126621" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3126594.3126621" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="pSZHUseWtj4" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_heatspace-automatic-placement-of-displays-by-empirical-analysis-of-user-behavior" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_heatspace-automatic-placement-of-displays-by-empirical-analysis-of-user-behavior">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Fender2017, 
 author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M"{u}ller, J"{o}rg}, 
 title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, 
 year = {2017}, 
 isbn = {9781450349819}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3126594.3126621}, 
 doi = {10.1145/3126594.3126621}, 
 booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, 
 pages = {611–621}, 
 numpages = {11}, 
 keywords = {multi-display environments, display placement, analyzing user behavior}, 
 location = {Quebec City, QC, Canada}, 
 series = {UIST '17} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2019-mentalimage.html&quot;,&quot;id&quot;:&quot;/publications/2019-mentalimage&quot;,&quot;relative_path&quot;:&quot;_publications/2019-mentalimage.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3290605.3300839&quot;,&quot;title&quot;:&quot;The Mental Image Revealed by Gaze Tracking&quot;,&quot;authors&quot;:[&quot;Xi Wang&quot;,&quot;Andreas Ley&quot;,&quot;Sebastian Koch&quot;,&quot;David Lindlbauer&quot;,&quot;James Hays&quot;,&quot;Kenneth Holmqvist&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://cybertron.cg.tu-berlin.de/xiwang/mental_imagery/retrieval.html&quot;,&quot;doi&quot;:&quot;10.1145/3290605.3300839&quot;,&quot;venue_location&quot;:&quot;Glasgow, UK&quot;,&quot;venue_url&quot;:&quot;https://chi2019.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Eye tracking&quot;,&quot;Gaze pattern&quot;,&quot;Mental imagery&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-30sec&quot;:&quot;gaKdA51v5dg&quot;,&quot;video-suppl&quot;:&quot;a7dBY_EZEUQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Wang19 \n author = {Wang, Xi and Ley, Andreas and Koch, Sebastian and Lindlbauer, David and Hays, James and Holmqvist, Kenneth and Alexa, Marc}, \n title = {The Mental Image Revealed by Gaze Tracking}, \n year = {2019}, \n isbn = {9781450359702}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3290605.3300839}, \n doi = {10.1145/3290605.3300839}, \n booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems}, \n pages = {1–12}, \n numpages = {12}, \n keywords = {gaze pattern, eye tracking, mental imagery}, \n location = {Glasgow, Scotland Uk}, \n series = {CHI '19} \n }&quot;,&quot;slug&quot;:&quot;2019-mentalimage&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;excerpt&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Context-Aware Online Adaptation of Mixed Reality Interfaces | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2019-compMR.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users’ current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;},\&quot;headline\&quot;:\&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2019-compMR.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Context-Aware Online Adaptation of Mixed Reality Interfaces\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Anna Maria Feit,\n          Otmar Hilliges.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2019/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2019\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2019-compMR.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an optimization-based approach for Mixed Reality (MR) systems to automatically control when and where applications are shown, and how much information they display. Currently, content creators design applications, and users then manually adjust which applications are visible and how much information they show. This choice has to be adjusted every time users switch context, i.e., whenever they switch their task or environment. Since context switches happen many times a day, we believe that MR interfaces require automation to alleviate this problem. We propose a real-time approach to automate this process based on users' current cognitive load and knowledge about their task and environment. Our system adapts which applications are displayed, how much information they show, and where they are placed. We formulate this problem as a mix of rule-based decision making and combinatorial optimization which can be solved efficiently in real-time. We present a set of proof-of-concept applications showing that our approach is applicable in a wide range of scenarios. Finally, we show in a dual-task evaluation that our approach decreased secondary tasks interactions by 36%.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/aGuwwWka0Fk?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3332165.3347945\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://ait.ethz.ch/projects/2019/computationalMR/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://github.com/eth-ait/ComputationalMR\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Code\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=aGuwwWka0Fk\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=heAGuCsWs9o\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=SAmmTUw96Mg\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2019-compMR.html&quot;,&quot;id&quot;:&quot;/publications/2019-compMR&quot;,&quot;relative_path&quot;:&quot;_publications/2019-compMR.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2019,&quot;month&quot;:10,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3332165.3347945&quot;,&quot;title&quot;:&quot;Context-Aware Online Adaptation of Mixed Reality Interfaces&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Anna Maria Feit&quot;,&quot;Otmar Hilliges&quot;],&quot;blog&quot;:&quot;https://ait.ethz.ch/projects/2019/computationalMR/&quot;,&quot;code&quot;:&quot;https://github.com/eth-ait/ComputationalMR&quot;,&quot;doi&quot;:&quot;10.1145/3332165.3347945&quot;,&quot;venue_location&quot;:&quot;New Orleans&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2019/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-30sec&quot;:&quot;aGuwwWka0Fk&quot;,&quot;video-suppl&quot;:&quot;heAGuCsWs9o&quot;,&quot;video-talk-15min&quot;:&quot;SAmmTUw96Mg&quot;,&quot;bibtex&quot;:&quot;@inproceedings { Lindlbauer2019a, \n author = {Lindlbauer, David and Feit, Anna Maria and Hilliges, Otmar}, \n title = {Context-Aware Online Adaptation of Mixed Reality Interfaces},\n year = {2019},\n isbn = {9781450368162},\n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/3332165.3347945},\n doi = {10.1145/3332165.3347945},\n pages = {147–160},\n numpages = {14},\n keywords = {ui optimization, context-awareness, mixed reality},\n location = {New Orleans, LA, USA},\n series = {UIST '19}\n }&quot;,&quot;slug&quot;:&quot;2019-compMR&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;excerpt&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Remixed Reality: Manipulating Space and Time in Augmented Reality | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;},\&quot;headline\&quot;:\&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2018-remixedreality.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Remixed Reality: Manipulating Space and Time in Augmented Reality\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Andrew D. Wilson.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2018.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2018\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2018-remixedreality.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/BjhaZi1l-hY?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3173574.3173703\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=BjhaZi1l-hY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=GoSQTPfrdCc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2017-illusions_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2017-illusions">
                  <a href="/publications/2017-illusions.html" class="black hover-cmu-red link">Changing the Appearance of Real-World Objects By Modifying Their Surroundings</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Jörg Müller, 
                  Marc Alexa.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2017
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2017-illusions.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/3025453.3025795" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/3025453.3025795" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="C-XO06wwQuY" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_changing-the-appearance-of-real-world-objects-by-modifying-their-surroundings" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_changing-the-appearance-of-real-world-objects-by-modifying-their-surroundings">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inbook{Lindlbauer2017, 
 author = {Lindlbauer, David and Mueller, J"{o}rg and Alexa, Marc}, 
 title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, 
 year = {2017}, 
 isbn = {9781450346559}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/3025453.3025795}, 
 booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, 
 pages = {3954–3965}, 
 numpages = {12} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2016">2016</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2016-tco_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2016-tco">
                  <a href="/publications/2016-tco.html" class="black hover-cmu-red link">Changing the Appearance of Physical Interfaces Through Controlled Transparency</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Jörg Müller, 
                  Marc Alexa.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2016
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2016-tco.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2984511.2984556" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2984511.2984556" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="f3e3SI-CKBM" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_changing-the-appearance-of-physical-interfaces-through-controlled-transparency" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_changing-the-appearance-of-physical-interfaces-through-controlled-transparency">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{Lindlbauer16a, 
 author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, 
 title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, 
 year = {2016}, 
 isbn = {9781450341899}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/2984511.2984556}, 
 doi = {10.1145/2984511.2984556}, 
 booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, 
 pages = {425–435}, 
 numpages = {11}, 
 keywords = {transparency control, dynamic appearance}, 
 location = {Tokyo, Japan}, 
 series = {UIST '16} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2018-remixedreality.html&quot;,&quot;id&quot;:&quot;/publications/2018-remixedreality&quot;,&quot;relative_path&quot;:&quot;_publications/2018-remixedreality.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2018,&quot;month&quot;:5,&quot;selected&quot;:true,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3173574.3173703&quot;,&quot;title&quot;:&quot;Remixed Reality: Manipulating Space and Time in Augmented Reality&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Andrew D. Wilson&quot;],&quot;blog&quot;:&quot;https://www.microsoft.com/en-us/research/blog/manipulating-space-time-mixed-reality/&quot;,&quot;doi&quot;:&quot;10.1145/3173574.3173703&quot;,&quot;venue_location&quot;:&quot;Montreal, Canada&quot;,&quot;venue_url&quot;:&quot;https://chi2018.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-30sec&quot;:&quot;BjhaZi1l-hY&quot;,&quot;video-suppl&quot;:&quot;GoSQTPfrdCc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer18, \n author = {Lindlbauer, David and Wilson, Andrew D.}, \n title = {Remixed Reality: Manipulating Space and Time in Augmented Reality}, \n year = {2018}, \n isbn = {9781450356206}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3173574.3173703}, \n doi = {10.1145/3173574.3173703}, \n pages = {1–13}, \n numpages = {13}, \n keywords = {remixed reality, virtual reality, augmented reality}, \n location = {Montreal QC, Canada}, \n series = {CHI '18} \n }&quot;,&quot;slug&quot;:&quot;2018-remixedreality&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;excerpt&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Real-World Objects By Modifying Their Surroundings | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-illusions.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-illusions.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Real-World Objects By Modifying Their Surroundings\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2017.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-illusions.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/bxWZP_m3PQQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3025453.3025795\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=bxWZP_m3PQQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C-XO06wwQuY\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-illusions.html&quot;,&quot;id&quot;:&quot;/publications/2017-illusions&quot;,&quot;relative_path&quot;:&quot;_publications/2017-illusions.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3025453.3025795&quot;,&quot;title&quot;:&quot;Changing the Appearance of Real-World Objects By Modifying Their Surroundings&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;doi&quot;:&quot;10.1145/3025453.3025795&quot;,&quot;venue_location&quot;:&quot;Denver, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2017.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-30sec&quot;:&quot;bxWZP_m3PQQ&quot;,&quot;video-suppl&quot;:&quot;C-XO06wwQuY&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer2017, \n author = {Lindlbauer, David and Mueller, J\&quot;{o}rg and Alexa, Marc}, \n title = {Changing the Appearance of Real-World Objects By Modifying Their Surroundings}, \n year = {2017}, \n isbn = {9781450346559}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3025453.3025795}, \n booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems}, \n pages = {3954–3965}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2017-illusions&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;excerpt&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users’ perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users’ field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;},\&quot;headline\&quot;:\&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2017-heatspace.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Philipp Herholz,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2017/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2017\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2017-heatspace.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present HeatSpace, a system that records and empirically analyzes user behavior in a space and automatically suggests positions and sizes for new displays. The system uses depth cameras to capture 3D geometry and users' perspectives over time. To derive possible display placements, it calculates volumetric heatmaps describing geometric persistence and planarity of structures inside the space. It evaluates visibility of display poses by calculating a volumetric heatmap describing occlusions, position within users' field of view, and viewing angle. Optimal display size is calculated through a heatmap of average viewing distance. Based on the heatmaps and user constraints we sample the space of valid display placements and jointly optimize their positions. This can be useful when installing displays in multi-display environments such as meeting rooms, offices, and train stations.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/9IQFY_fNz_w?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/3126594.3126621\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=9IQFY_fNz_w\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=pSZHUseWtj4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=A-1S7OexOQA\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2016-transparentdisplay_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2016-transparentdisplay">
<i class="fas fa-award cmu-red" title="Best Paper Honorable Mention Award"></i> 
                  <a href="/publications/2016-transparentdisplay.html" class="black hover-cmu-red link">Influence of Display Transparency on Background Awareness and Task Performance</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Klemen Lilija, 
                  Robert Walter, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2016
                </a>
                </span>
              </div>

              

              

              
              <div class="cmu-red mv1 b">
                Best Paper Honorable Mention Award
                
              </div>
              

              <div class="mt2 mb1">
                <a href="/publications/2016-transparentdisplay.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2858036.2858453" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2858036.2858453" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                <a href="https://www.cg.tu-berlin.de/research/projects/sci-and-ar/" class="black link underline-dot hover-cmu-red mr3 dib">
                  Blog
                </a>
                
                                
                
                <a href="8wWlO97V_OM" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_influence-of-display-transparency-on-background-awareness-and-task-performance" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_influence-of-display-transparency-on-background-awareness-and-task-performance">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inbook{Lindlbauer16c, 
 author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M"{u}ller, J"{o}rg}, 
 title = {Influence of Display Transparency on Background Awareness and Task Performance}, 
 year = {2016}, 
 isbn = {9781450333627}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/2858036.2858453}, 
 pages = {1705–1716}, 
 numpages = {12} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2017-heatspace.html&quot;,&quot;id&quot;:&quot;/publications/2017-heatspace&quot;,&quot;relative_path&quot;:&quot;_publications/2017-heatspace.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2017,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/3126594.3126621&quot;,&quot;title&quot;:&quot;HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;David Lindlbauer&quot;,&quot;Philipp Herholz&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/3126594.3126621&quot;,&quot;venue_location&quot;:&quot;Quebec City, Canada&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2017/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Perception&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-30sec&quot;:&quot;9IQFY_fNz_w&quot;,&quot;video-suppl&quot;:&quot;pSZHUseWtj4&quot;,&quot;video-talk-15min&quot;:&quot;A-1S7OexOQA&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Fender2017, \n author = {Fender, Andreas and Lindlbauer, David and Herholz, Philipp and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {HeatSpace: Automatic Placement of Displays by Empirical Analysis of User Behavior}, \n year = {2017}, \n isbn = {9781450349819}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/3126594.3126621}, \n doi = {10.1145/3126594.3126621}, \n booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {611–621}, \n numpages = {11}, \n keywords = {multi-display environments, display placement, analyzing user behavior}, \n location = {Quebec City, QC, Canada}, \n series = {UIST '17} \n }&quot;,&quot;slug&quot;:&quot;2017-heatspace&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;excerpt&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Influence of Display Transparency on Background Awareness and Task Performance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;},\&quot;headline\&quot;:\&quot;Influence of Display Transparency on Background Awareness and Task Performance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-transparentdisplay.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Influence of Display Transparency on Background Awareness and Task Performance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Klemen Lilija,\n          Robert Walter,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-transparentdisplay.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        It has been argued that transparent displays are beneficial for certain tasks by allowing users to simultaneously see on-screen content as well as the environment behind the display. However, it is yet unclear how much in background awareness users gain and if performance suffers for tasks performed on the transparent display, since users are no longer shielded from distractions. Therefore, we investigate the influence of display transparency on task performance and background awareness in a dual-task scenario. We conducted an experiment comparing transparent displays with conventional displays in different horizontal and vertical configurations. Participants performed an attention-demanding primary task on the display while simultaneously observing the background for target stimuli. Our results show that transparent and horizontal displays increase the ability of participants to observe the background while keeping primary task performance constant.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/OATMC0odHrE?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858453\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=OATMC0odHrE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=8wWlO97V_OM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2016-sci_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2016-sci">
                  <a href="/publications/2016-sci.html" class="black hover-cmu-red link">Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Jens Emil Grønbæk, 
                  Morten Birk, 
                  Kim Halskov, 
                  Marc Alexa, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2016
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2016-sci.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2858036.2858457" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2858036.2858457" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="fWREdKL2Kus" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_combining-shape-changing-interfaces-and-spatial-augmented-reality-enables-extended-object-appearance" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_combining-shape-changing-interfaces-and-spatial-augmented-reality-enables-extended-object-appearance">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inbook{Lindlbauer16b, 
 author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M"{u}ller, J"{o}rg}, 
 title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, 
 year = {2016}, 
 isbn = {9781450333627}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/2858036.2858457}, 
 booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, 
 pages = {791–802}, 
 numpages = {12} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2015">2015</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2015-geltouch_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2015-geltouch">
<i class="fas fa-award cmu-red" title="Best Paper Honorable Mention Award"></i> 
                  <a href="/publications/2015-geltouch.html" class="black hover-cmu-red link">GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel</a>
</h3>
              
                  <div class="mb2">
                
                  Viktor Miruchna, 
                  Robert Walter, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Maren Lehmann, 
                  Regine von Klitzing, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2015
                </a>
                </span>
              </div>

              

              

              
              <div class="cmu-red mv1 b">
                Best Paper Honorable Mention Award
                
              </div>
              

              <div class="mt2 mb1">
                <a href="/publications/2015-geltouch.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2807442.2807487" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2807442.2807487" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="C40bl9qmLV0" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_geltouch-localized-tactile-feedback-through-thin-programmable-gel" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_geltouch-localized-tactile-feedback-through-thin-programmable-gel">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2807442.2807487,
 author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M"{u}ller, J"{o}rg},
 title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},
 year = {2015},
 isbn = {9781450337793},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2807442.2807487},
 doi = {10.1145/2807442.2807487},
 booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp; Technology},
 pages = {3–10},
 numpages = {8},
 keywords = {thermoresponsive hydrogel, tactile feedback},
 location = {Charlotte, NC, USA},
 series = {UIST '15}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2015-creature-teacher_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/NvyihtlxIKQ?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2015-creature-teacher">
                  <a href="/publications/2015-creature-teacher.html" class="black hover-cmu-red link">Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements</a>
</h3>
              
                  <div class="mb2">
                
                  Andreas Fender, 
                  Jörg Müller, 
                  David Lindlbauer..
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                SUI
                2015
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2015-creature-teacher.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2788940.2788944" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2788940.2788944" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="NvyihtlxIKQ" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_creature-teacher-a-performance-based-animation-system-for-creating-cyclic-movements" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_creature-teacher-a-performance-based-animation-system-for-creating-cyclic-movements">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2788940.2788944,
 author = {Fender, Andreas and M"{u}ller, J"{o}rg and Lindlbauer, David},
 title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},
 year = {2015},
 isbn = {9781450337038},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2788940.2788944},
 doi = {10.1145/2788940.2788944},
 booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},
 pages = {113–122},
 numpages = {10},
 keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},
 location = {Los Angeles, California, USA},
 series = {SUI '15}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-transparentdisplay.html&quot;,&quot;id&quot;:&quot;/publications/2016-transparentdisplay&quot;,&quot;relative_path&quot;:&quot;_publications/2016-transparentdisplay.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858453&quot;,&quot;title&quot;:&quot;Influence of Display Transparency on Background Awareness and Task Performance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Klemen Lilija&quot;,&quot;Robert Walter&quot;,&quot;Jörg Müller&quot;],&quot;blog&quot;:&quot;https://www.cg.tu-berlin.de/research/projects/sci-and-ar/&quot;,&quot;doi&quot;:&quot;10.1145/2858036.2858453&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;OATMC0odHrE&quot;,&quot;video-30sec&quot;:&quot;OATMC0odHrE&quot;,&quot;video-suppl&quot;:&quot;8wWlO97V_OM&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16c, \n author = {Lindlbauer, David and Lilija, Klemen and Walter, Robert and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Influence of Display Transparency on Background Awareness and Task Performance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858453}, \n pages = {1705–1716}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-transparentdisplay&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;excerpt&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;&lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br/&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br/&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n&lt;/p&gt;\n\n&lt;p&gt;\nPDF:&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br/&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br/&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Changing the Appearance of Physical Interfaces Through Controlled Transparency | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-tco.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;},\&quot;headline\&quot;:\&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-tco.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Changing the Appearance of Physical Interfaces Through Controlled Transparency\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jörg Müller,\n          Marc Alexa.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2016/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-tco.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        &lt;p&gt;\nWe present physical interfaces that change their appearance through controlled transparency. These transparency-controlled physical interfaces are well suited for applications where communication through optical appearance is sufficient, such as ambient display scenarios. They transition between perceived shapes within milliseconds, require no mechanically moving parts and consume little energy. We build 3D physical interfaces with individually controllable parts by laser cutting and folding a single sheet of transparency-controlled material. Electrical connections are engraved in the surface, eliminating the need for wiring individual parts. We consider our work as complementary to current shape-changing interfaces. While our proposed interfaces do not exhibit dynamic tangible qualities, they have unique benefits such as the ability to create apparent holes or nesting of objects. We explore the benefits of transparency-controlled physical interfaces by characterizing their design space and showcase four physical prototypes: two activity indicators, a playful avatar, and a lamp shade with dynamic appearance.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Disclaimer&lt;/strong&gt;: Software and hardware is provided as is. Feel free to use it, be we do not give any kind of warranty. Electrical schematics should be used with caution (i.e. use it at own risk). Do not try to build, wire or solder anything if you don’t feel confident in what you are doing. The software is most likely to contain bugs (known and unknown). Treat it more like a dictionary that should help you with your own implementation.\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Source code&lt;/strong&gt;&lt;br&gt;\nThe software is written in c++ (with Xcode, runs on OSX) with openFrameworks for GUI and algorithms. Below, you can download the software as zip, bundled with openFramework 0.9. It should run pretty much out of the box. We might put it on GitHub in the future. This would, however, require a major cleanup and refactoring first.\n&lt;/p&gt;\n\n&lt;p&gt;\nDownload (130MB, all frameworks included):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/origami.zip\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Schematics&lt;/strong&gt;&lt;br&gt;\nWe designed a board that takes 1 line of AC as input and splits it in 8 for addressing the individual parts of an object. Each board is controlled with a shift register, connected to an Arduino. The boards can be changed for supporting more than 8 parts or multiple objects. Do not use this unless you know what you’re doing!!! The circuit should probably be re-designed for safety.\n&lt;/p&gt;\nEagle (50KB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac-export.zip\n\n\n&lt;p&gt;\nPDF:&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/patches-ac.pdf\n&lt;/p&gt;\n\n&lt;p&gt;\n&lt;strong&gt;Examples&lt;/strong&gt;&lt;br&gt;\nThe examples were constructed with our software and post-processed in Adobe Illustrator. They contain the outlines and routing for laser cutting the transparency-controlled physical interfaces shown in the paper. The routing should only cut 1 layer of ITO from the switchable diffuser, while the outlines cuts through all layers. Our settings for laser cutting are 100% speed / 12% power for cutting the outline and 100% speed / 5% power for the routing (Epilog Zing 24).\n&lt;/p&gt;\n\n&lt;p&gt;\nAdobe Illustrator (2MB):&lt;br&gt;\nhttp://cybertron.cg.tu-berlin.de/dalind/tco/examples.zip\n&lt;/p&gt;\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/DR6R-hSCuqQ?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2984511.2984556\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Blog\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=DR6R-hSCuqQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=f3e3SI-CKBM\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=ABvP_wpvMSI\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-tco.html&quot;,&quot;id&quot;:&quot;/publications/2016-tco&quot;,&quot;relative_path&quot;:&quot;_publications/2016-tco.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2984511.2984556&quot;,&quot;title&quot;:&quot;Changing the Appearance of Physical Interfaces Through Controlled Transparency&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jörg Müller&quot;,&quot;Marc Alexa&quot;],&quot;blog&quot;:&quot;http://www.cg.tu-berlin.de/research/projects/transparency-controlled-physical-interfaces/&quot;,&quot;doi&quot;:&quot;10.1145/2984511.2984556&quot;,&quot;venue_location&quot;:&quot;Tokyo, Japan&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2016/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-30sec&quot;:&quot;DR6R-hSCuqQ&quot;,&quot;video-suppl&quot;:&quot;f3e3SI-CKBM&quot;,&quot;video-talk-15min&quot;:&quot;ABvP_wpvMSI&quot;,&quot;bibtex&quot;:&quot;@inproceedings{Lindlbauer16a, \n author = {Lindlbauer, David and Mueller, Joerg and Alexa, Marc}, \n title = {Changing the Appearance of Physical Interfaces Through Controlled Transparency}, \n year = {2016}, \n isbn = {9781450341899}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2984511.2984556}, \n doi = {10.1145/2984511.2984556}, \n booktitle = {Proceedings of the 29th Annual Symposium on User Interface Software and Technology}, \n pages = {425–435}, \n numpages = {11}, \n keywords = {transparency control, dynamic appearance}, \n location = {Tokyo, Japan}, \n series = {UIST '16} \n }&quot;,&quot;slug&quot;:&quot;2016-tco&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;excerpt&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2016-sci.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects’ optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;},\&quot;headline\&quot;:\&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2016-sci.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2016.acm.org/wp/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2016\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2016-sci.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/uyvBJqv3s_M?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2858036.2858457\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=uyvBJqv3s_M\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=fWREdKL2Kus\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=28_DwAVaKgE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2016-sci.html&quot;,&quot;id&quot;:&quot;/publications/2016-sci&quot;,&quot;relative_path&quot;:&quot;_publications/2016-sci.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2016,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2858036.2858457&quot;,&quot;title&quot;:&quot;Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2858036.2858457&quot;,&quot;venue_location&quot;:&quot;San Jose, USA&quot;,&quot;venue_url&quot;:&quot;https://chi2016.acm.org/wp/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Shape-changing interfaces&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-30sec&quot;:&quot;uyvBJqv3s_M&quot;,&quot;video-suppl&quot;:&quot;fWREdKL2Kus&quot;,&quot;video-talk-15min&quot;:&quot;28_DwAVaKgE&quot;,&quot;bibtex&quot;:&quot;@inbook{Lindlbauer16b, \n author = {Lindlbauer, David and Grønbæk, Jens Emil and Birk, Morten and Halskov, Kim and Alexa, Marc and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance}, \n year = {2016}, \n isbn = {9781450333627}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2858036.2858457}, \n booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems}, \n pages = {791–802}, \n numpages = {12} \n }&quot;,&quot;slug&quot;:&quot;2016-sci&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;excerpt&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Analyzing visual attention during whole body interaction with public displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Robert Walter\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-public-display.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Analyzing visual attention during whole body interaction with public displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users’ ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users’ movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users’ silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;},\&quot;headline\&quot;:\&quot;Analyzing visual attention during whole body interaction with public displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-public-display.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Robert Walter\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Analyzing visual attention during whole body interaction with public displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Robert Walter,\n          Andreas Bulling,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Martin Schuessler,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Robert Walter\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Robert Walter&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://ubicomp.org/ubicomp2015/index.html\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UBICOMP\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-public-display.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        While whole body interaction can enrich user experience on public displays, it remains unclear how common visualizations of user representations impact users' ability to perceive content on the display. In this work we use a head-mounted eye tracker to record visual behavior of 25 users interacting with a public display game that uses a silhouette user representation, mirroring the users' movements. Results from visual attention analysis as well as post-hoc recall and recognition tasks on display contents reveal that visual attention is mostly on users' silhouette while peripheral screen elements remain largely unattended. In our experiment, content attached to the user representation attracted significantly more attention than other screen contents, while content placed at the top and bottom of the screen attracted significantly less. Screen contents attached to the user representation were also significantly better remembered than those at the top and bottom of the screen.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2750858.2804255\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=gLzqtUE87v8\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2015-public-display_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/gLzqtUE87v8?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2015-public-display">
                  <a href="/publications/2015-public-display.html" class="black hover-cmu-red link">Analyzing visual attention during whole body interaction with public displays</a>
</h3>
              
                  <div class="mb2">
                
                  Robert Walter, 
                  Andreas Bulling, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Martin Schuessler, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UBICOMP
                2015
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2015-public-display.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2750858.2804255" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2750858.2804255" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="gLzqtUE87v8" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_analyzing-visual-attention-during-whole-body-interaction-with-public-displays" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_analyzing-visual-attention-during-whole-body-interaction-with-public-displays">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2750858.2804255,
 author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M"{u}ller, J"{o}rg},
 title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},
 year = {2015},
 isbn = {9781450335744},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2750858.2804255},
 doi = {10.1145/2750858.2804255},
 booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
 pages = {1263–1267},
 numpages = {5},
 keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},
 location = {Osaka, Japan},
 series = {UbiComp '15}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2014">2014</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-public-display.html&quot;,&quot;id&quot;:&quot;/publications/2015-public-display&quot;,&quot;relative_path&quot;:&quot;_publications/2015-public-display.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:8,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2750858.2804255&quot;,&quot;title&quot;:&quot;Analyzing visual attention during whole body interaction with public displays&quot;,&quot;authors&quot;:[&quot;Robert Walter&quot;,&quot;Andreas Bulling&quot;,&quot;David Lindlbauer&quot;,&quot;Martin Schuessler&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2750858.2804255&quot;,&quot;venue_location&quot;:&quot;Osaka, Japan&quot;,&quot;venue_url&quot;:&quot;https://ubicomp.org/ubicomp2015/index.html&quot;,&quot;venue_tags&quot;:[&quot;UBICOMP&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Visual attention&quot;,&quot;Public displays&quot;],&quot;venue&quot;:&quot;UBICOMP&quot;,&quot;video-thumb&quot;:&quot;gLzqtUE87v8&quot;,&quot;video-suppl&quot;:&quot;gLzqtUE87v8&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2750858.2804255,\n author = {Walter, Robert and Bulling, Andreas and Lindlbauer, David and Schuessler, Martin and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {Analyzing Visual Attention during Whole Body Interaction with Public Displays},\n year = {2015},\n isbn = {9781450335744},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2750858.2804255},\n doi = {10.1145/2750858.2804255},\n booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},\n pages = {1263–1267},\n numpages = {5},\n keywords = {mobile eye tracking, whole body interaction, user representation, visual attention, public displays},\n location = {Osaka, Japan},\n series = {UbiComp '15}\n }&quot;,&quot;slug&quot;:&quot;2015-public-display&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;excerpt&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Viktor Miruchna\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;},\&quot;headline\&quot;:\&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-geltouch.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Viktor Miruchna\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Viktor Miruchna,\n          Robert Walter,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Maren Lehmann,\n          Regine von Klitzing,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Viktor Miruchna\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Viktor Miruchna&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          \n          &lt;li class=\&quot;mt1 cmu-red\&quot;&gt;\n              &lt;i class=\&quot;fas fa-award\&quot;&gt;&lt;/i&gt; Best Paper Honorable Mention Award\n          &lt;/li&gt;\n          \n        &lt;/ul&gt;\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-geltouch.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (&amp;gt;32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/o8W6qbwPhwU?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2807442.2807487\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=o8W6qbwPhwU\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=C40bl9qmLV0\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=FQeS6ASPnh4\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;excerpt&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;previous&quot;:null,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Perceptual grouping: selection assistance for digital sketching | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;},\&quot;headline\&quot;:\&quot;Perceptual grouping: selection assistance for digital sketching\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Perceptual grouping: selection assistance for digital sketching\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Mark Hancock,\n          Stacey D. Scott,\n          Wolfgang Stuerzlinger.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://its2013.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2013\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2013-suggero.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2512349.2512801\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=oPa7bATFbDc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2014-tracs_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2014-tracs">
                  <a href="/publications/2014-tracs.html" class="black hover-cmu-red link">Tracs: transparency-control for see-through displays</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Jens Emil Grønbæk, 
                  Morten Birk, 
                  Kim Halskov, 
                  Marc Alexa, 
                  Jörg Müller.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2014
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2014-tracs.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2642918.2647350" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2642918.2647350" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="huNZJcyryfE" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_tracs-transparency-control-for-see-through-displays" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_tracs-transparency-control-for-see-through-displays">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2642918.2647350, 
 author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H"{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M"{u}ller, J"{o}rg}, 
 title = {Tracs: Transparency-Control for See-through Displays}, 
 year = {2014}, 
 isbn = {9781450330695}, 
 publisher = {Association for Computing Machinery}, 
 address = {New York, NY, USA}, 
 url = {https://doi.org/10.1145/2642918.2647350}, 
 doi = {10.1145/2642918.2647350}, 
 booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, 
 pages = {657–661}, 
 numpages = {5}, 
 keywords = {transparency-controlled display, transparent display}, 
 location = {Honolulu, Hawaii, USA}, 
 series = {UIST '14} 
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-geltouch.html&quot;,&quot;id&quot;:&quot;/publications/2015-geltouch&quot;,&quot;relative_path&quot;:&quot;_publications/2015-geltouch.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2807442.2807487&quot;,&quot;title&quot;:&quot;GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel&quot;,&quot;authors&quot;:[&quot;Viktor Miruchna&quot;,&quot;Robert Walter&quot;,&quot;David Lindlbauer&quot;,&quot;Maren Lehmann&quot;,&quot;Regine von Klitzing&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2807442.2807487&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2015/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Haptics&quot;,&quot;Hydrogel&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;awards&quot;:&quot;Best Paper Honorable Mention Award&quot;,&quot;video-thumb&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-30sec&quot;:&quot;o8W6qbwPhwU&quot;,&quot;video-suppl&quot;:&quot;C40bl9qmLV0&quot;,&quot;video-talk-15min&quot;:&quot;FQeS6ASPnh4&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2807442.2807487,\n author = {Miruchna, Viktor and Walter, Robert and Lindlbauer, David and Lehmann, Maren and von Klitzing, Regine and M\&quot;{u}ller, J\&quot;{o}rg},\n title = {GelTouch: Localized Tactile Feedback Through Thin, Programmable Gel},\n year = {2015},\n isbn = {9781450337793},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2807442.2807487},\n doi = {10.1145/2807442.2807487},\n booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software &amp;amp; Technology},\n pages = {3–10},\n numpages = {8},\n keywords = {thermoresponsive hydrogel, tactile feedback},\n location = {Charlotte, NC, USA},\n series = {UIST '15}\n }&quot;,&quot;slug&quot;:&quot;2015-geltouch&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;excerpt&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Andreas Fender\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher’s generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;},\&quot;headline\&quot;:\&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2015-creature-teacher.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Andreas Fender\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Andreas Fender,\n          Jörg Müller,\n          David Lindlbauer..\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Andreas Fender\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Andreas Fender&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;http://sui.acm.org/2015/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            SUI\n            2015\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2015-creature-teacher.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Creature Teacher, a performance-based animation system for creating cyclic movements. Users directly manipulate body parts of a virtual character by using their hands. Creature Teacher's generic approach makes it possible to animate rigged 3D models with nearly arbitrary topology (e.g., non-humanoid) without requiring specialized user-to-character mappings or predefined movements. We use a bimanual interaction paradigm, allowing users to select parts of the model with one hand and manipulate them with the other hand. Cyclic movements of body parts during manipulation are detected and repeatedly played back - also while animating other body parts. Our approach of taking cyclic movements as an input makes mode switching between recording and playback obsolete and allows for fast and seamless creation of animations. We show that novice users with no animation background were able to create expressive cyclic animations for initially static virtual 3D creatures.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2788940.2788944\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=NvyihtlxIKQ\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;excerpt&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;previous&quot;:null,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Perceptual grouping: selection assistance for digital sketching | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;},\&quot;headline\&quot;:\&quot;Perceptual grouping: selection assistance for digital sketching\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Perceptual grouping: selection assistance for digital sketching\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Mark Hancock,\n          Stacey D. Scott,\n          Wolfgang Stuerzlinger.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://its2013.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2013\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2013-suggero.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2512349.2512801\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=oPa7bATFbDc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2014-chair_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2014-chair">
                  <a href="/publications/2014-chair.html" class="black hover-cmu-red link">A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction</a>
</h3>
              
                  <div class="mb2">
                
                  Kathrin Probst, 
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Michael Haller, 
                  Bernhard Schwartz, 
                  Andreas Schrempf.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                CHI
                2014
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2014-chair.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2556288.2557051" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2556288.2557051" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                
                <!-- 
                 -->

                
                <label for="slide_a-chair-as-ubiquitous-input-device-exploring-semaphoric-chair-gestures-for-focused-and-peripheral-interaction" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_a-chair-as-ubiquitous-input-device-exploring-semaphoric-chair-gestures-for-focused-and-peripheral-interaction">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2556288.2557051,
 author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},
 title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},
 year = {2014},
 isbn = {9781450324731},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2556288.2557051},
 doi = {10.1145/2556288.2557051},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 pages = {4097–4106},
 numpages = {10},
 keywords = {gestural interaction, interactive chair, input technologies},
 location = {Toronto, Ontario, Canada},
 series = {CHI '14}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
    <h2 class="year f3" id="y2013">2013</h2>  
      

    
      
      
      <div class="publication flex mt3" data-pub="{&quot;next&quot;:{&quot;next&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2015-creature-teacher.html&quot;,&quot;id&quot;:&quot;/publications/2015-creature-teacher&quot;,&quot;relative_path&quot;:&quot;_publications/2015-creature-teacher.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2015,&quot;month&quot;:9,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2788940.2788944&quot;,&quot;title&quot;:&quot;Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements&quot;,&quot;authors&quot;:[&quot;Andreas Fender&quot;,&quot;Jörg Müller&quot;,&quot;David Lindlbauer.&quot;],&quot;doi&quot;:&quot;10.1145/2788940.2788944&quot;,&quot;venue_location&quot;:&quot;Charlotte, NC, USA&quot;,&quot;venue_url&quot;:&quot;http://sui.acm.org/2015/&quot;,&quot;venue_tags&quot;:[&quot;SUI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Virtual Reality&quot;,&quot;Interaction techniques&quot;],&quot;venue&quot;:&quot;SUI&quot;,&quot;video-thumb&quot;:&quot;NvyihtlxIKQ&quot;,&quot;video-suppl&quot;:&quot;NvyihtlxIKQ&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2788940.2788944,\n author = {Fender, Andreas and M\&quot;{u}ller, J\&quot;{o}rg and Lindlbauer, David},\n title = {Creature Teacher: A Performance-Based Animation System for Creating Cyclic Movements},\n year = {2015},\n isbn = {9781450337038},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2788940.2788944},\n doi = {10.1145/2788940.2788944},\n booktitle = {Proceedings of the 3rd ACM Symposium on Spatial User Interaction},\n pages = {113–122},\n numpages = {10},\n keywords = {animation, performance-based, 3d user interface, 3d interaction and graphics, virtual reality},\n location = {Los Angeles, California, USA},\n series = {SUI '15}\n }&quot;,&quot;slug&quot;:&quot;2015-creature-teacher&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;excerpt&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n&quot;,&quot;previous&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Tracs: transparency-control for see-through displays | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-tracs.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Tracs: transparency-control for see-through displays\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;},\&quot;headline\&quot;:\&quot;Tracs: transparency-control for see-through displays\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-tracs.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Tracs: transparency-control for see-through displays\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Jens Emil Grønbæk,\n          Morten Birk,\n          Kim Halskov,\n          Marc Alexa,\n          Jörg Müller.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://uist.acm.org/uist2014/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-tracs.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        We present Tracs, a dual-sided see-through display system with controllable transparency. Traditional displays are a constant visual and communication barrier, hindering fast and efficient collaboration of spatially close or facing co-workers. Transparent displays could potentially remove these barriers, but introduce new issues of personal privacy, screen content privacy and visual interference. We therefore propose a solution with controllable transparency to overcome these problems. Tracs consists of two see-through displays, with a transparency-control layer, a backlight layer and a polarization adjustment layer in-between. The transparency-control layer is built as a grid of individually addressable transparency-controlled patches, allowing users to control the transparency overall or just locally. Additionally, the locally switchable backlight layer improves the contrast of LCD screen content. Tracs allows users to switch between personal and collaborative work fast and easily and gives them full control of transparent regions on their display.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/d9zx_WTvJ3A?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2642918.2647350\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=d9zx_WTvJ3A\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=huNZJcyryfE\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-tracs.html&quot;,&quot;id&quot;:&quot;/publications/2014-tracs&quot;,&quot;relative_path&quot;:&quot;_publications/2014-tracs.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2642918.2647350&quot;,&quot;title&quot;:&quot;Tracs: transparency-control for see-through displays&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Jens Emil Grønbæk&quot;,&quot;Morten Birk&quot;,&quot;Kim Halskov&quot;,&quot;Marc Alexa&quot;,&quot;Jörg Müller&quot;],&quot;doi&quot;:&quot;10.1145/2642918.2647350&quot;,&quot;venue_location&quot;:&quot;Hawaii, USA&quot;,&quot;venue_url&quot;:&quot;https://uist.acm.org/uist2014/&quot;,&quot;venue_tags&quot;:[&quot;UIST&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Mixed Reality&quot;,&quot;Transparent displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-30sec&quot;:&quot;d9zx_WTvJ3A&quot;,&quot;video-suppl&quot;:&quot;huNZJcyryfE&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2642918.2647350, \n author = {Lindlbauer, David and Aoki, Toru and Walter, Robert and Uema, Yuji and H\&quot;{o}chtl, Anita and Haller, Michael and Inami, Masahiko and M\&quot;{u}ller, J\&quot;{o}rg}, \n title = {Tracs: Transparency-Control for See-through Displays}, \n year = {2014}, \n isbn = {9781450330695}, \n publisher = {Association for Computing Machinery}, \n address = {New York, NY, USA}, \n url = {https://doi.org/10.1145/2642918.2647350}, \n doi = {10.1145/2642918.2647350}, \n booktitle = {Proceedings of the 27th Annual ACM Symposium on User Interface Software and Technology}, \n pages = {657–661}, \n numpages = {5}, \n keywords = {transparency-controlled display, transparent display}, \n location = {Honolulu, Hawaii, USA}, \n series = {UIST '14} \n }&quot;,&quot;slug&quot;:&quot;2014-tracs&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;excerpt&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.&quot;,&quot;previous&quot;:{&quot;next&quot;:{&quot;path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;excerpt&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;previous&quot;:null,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Perceptual grouping: selection assistance for digital sketching | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;},\&quot;headline\&quot;:\&quot;Perceptual grouping: selection assistance for digital sketching\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Perceptual grouping: selection assistance for digital sketching\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Mark Hancock,\n          Stacey D. Scott,\n          Wolfgang Stuerzlinger.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://its2013.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2013\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2013-suggero.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2512349.2512801\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=oPa7bATFbDc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;Kathrin Probst\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2014-chair.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;},\&quot;headline\&quot;:\&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2014-chair.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;Kathrin Probst\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          Kathrin Probst,\n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Bernhard Schwartz,\n          Andreas Schrempf.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of Kathrin Probst\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;Kathrin Probst&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://chi2014.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            CHI\n            2014\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2014-chair.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        During everyday office work we are used to controlling our computers with keyboard and mouse, while the majority of our body remains unchallenged and the physical workspace around us stays largely unattended. Addressing this untapped potential, we explore the concept of turning a flexible office chair into a ubiquitous input device. To facilitate daily desktop work, we propose the utilization of semaphoric chair gestures that can be assigned to specific application functionalities. The exploration of two usage scenarios in the context of focused and peripheral interaction demonstrates high potential of chair gestures as additional input modality for opportunistic, hands-free interaction.\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n    &lt;iframe width=\&quot;600\&quot; height=\&quot;360\&quot; class=\&quot;thumb-video mt3 mb2\&quot; src=\&quot;https://www.youtube.com/embed/roDd3n0Zjss?iv_load_policy=3&amp;amp;modestbranding=1&amp;amp;rel=0&amp;amp;autohide=1&amp;amp;playsinline=1&amp;amp;controls=1&amp;amp;showinfo=0&amp;amp;autoplay=0&amp;amp;loop=0&amp;amp;mute=1\&quot; frameborder=\&quot;0\&quot; allowfullscreen&gt;\n    &lt;/iframe&gt;   \n\n    &lt;!-- &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n      Video: 30sec teaser\n    &lt;/a&gt;&lt;/li&gt; --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2556288.2557051\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=roDd3n0Zjss\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: 30sec teaser\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=Mgx-B3sn218\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Long presentation\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2014-chair.html&quot;,&quot;id&quot;:&quot;/publications/2014-chair&quot;,&quot;relative_path&quot;:&quot;_publications/2014-chair.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2014,&quot;month&quot;:5,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2556288.2557051&quot;,&quot;title&quot;:&quot;A chair as ubiquitous input device: exploring semaphoric chair gestures for focused and peripheral interaction&quot;,&quot;authors&quot;:[&quot;Kathrin Probst&quot;,&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Bernhard Schwartz&quot;,&quot;Andreas Schrempf&quot;],&quot;doi&quot;:&quot;10.1145/2556288.2557051&quot;,&quot;venue_location&quot;:&quot;Paris, France&quot;,&quot;venue_url&quot;:&quot;https://chi2014.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;CHI&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Peripheral interaction&quot;,&quot;Embodied interaction&quot;],&quot;venue&quot;:&quot;CHI&quot;,&quot;video-thumb&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-30sec&quot;:&quot;roDd3n0Zjss&quot;,&quot;video-talk-15min&quot;:&quot;Mgx-B3sn218&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2556288.2557051,\n author = {Probst, Kathrin and Lindlbauer, David and Haller, Michael and Schwartz, Bernhard and Schrempf, Andreas},\n title = {A Chair as Ubiquitous Input Device: Exploring Semaphoric Chair Gestures for Focused and Peripheral Interaction},\n year = {2014},\n isbn = {9781450324731},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2556288.2557051},\n doi = {10.1145/2556288.2557051},\n booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},\n pages = {4097–4106},\n numpages = {10},\n keywords = {gestural interaction, interactive chair, input technologies},\n location = {Toronto, Ontario, Canada},\n series = {CHI '14}\n }&quot;,&quot;slug&quot;:&quot;2014-chair&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;},&quot;path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;excerpt&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;collection&quot;:&quot;publications&quot;,&quot;content&quot;:&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n&quot;,&quot;previous&quot;:null,&quot;output&quot;:&quot;&lt;!DOCTYPE html&gt;\n&lt;html lang=\&quot;en\&quot;&gt;\n  &lt;head&gt;\n  &lt;meta charset=\&quot;utf-8\&quot; /&gt;\n  &lt;meta http-equiv=\&quot;x-ua-compatible\&quot; content=\&quot;ie=edge\&quot; /&gt;\n  &lt;meta name=\&quot;viewport\&quot; content=\&quot;width=device-width, initial-scale=1\&quot; /&gt;\n  &lt;meta name=\&quot;application-name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n  &lt;meta name=\&quot;theme-color\&quot; content=\&quot;#b00\&quot; /&gt;\n  \n  &lt;link href=\&quot;https://fonts.gstatic.com\&quot; rel=\&quot;preconnect\&quot; crossorigin /&gt;\n  &lt;link\n    href=\&quot;https://fonts.googleapis.com/css?family=Open+Sans:300,400,700&amp;display=swap\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  \n  &lt;link href=\&quot;https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&amp;display=swap\&quot; rel=\&quot;stylesheet\&quot;&gt;\n\n  &lt;link\n    href=\&quot;https://use.fontawesome.com/releases/v5.13.0/css/all.css\&quot;\n    rel=\&quot;stylesheet\&quot;\n  /&gt;\n  &lt;link href=\&quot;/styles.css\&quot; rel=\&quot;stylesheet\&quot; /&gt;\n  \n  &lt;link rel=\&quot;stylesheet\&quot; href=\&quot;https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\&quot;&gt;\n\n  &lt;link rel=\&quot;shortcut icon\&quot; href=\&quot;/favicon.ico\&quot; /&gt;\n  &lt;link\n    rel=\&quot;icon\&quot;\n    type=\&quot;image/png\&quot;\n    href=\&quot;/assets/logo-light.png\&quot;\n    sizes=\&quot;250x250\&quot;\n  /&gt;\n\n  &lt;link\n    rel=\&quot;alternate\&quot;\n    type=\&quot;application/rss+xml\&quot;\n    title=\&quot;CMU Augmented Perception Lab\&quot;\n    href=\&quot;http://localhost:4000/feed.xml\&quot;\n  /&gt;\n\n  &lt;!-- Begin Jekyll SEO tag v2.7.1 --&gt;\n&lt;title&gt;Perceptual grouping: selection assistance for digital sketching | CMU Augmented Perception Lab&lt;/title&gt;\n&lt;meta name=\&quot;generator\&quot; content=\&quot;Jekyll v3.9.0\&quot; /&gt;\n&lt;meta property=\&quot;og:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;meta name=\&quot;author\&quot; content=\&quot;David Lindlbauer\&quot; /&gt;\n&lt;meta property=\&quot;og:locale\&quot; content=\&quot;en_US\&quot; /&gt;\n&lt;meta name=\&quot;description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;meta property=\&quot;og:description\&quot; content=\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot; /&gt;\n&lt;link rel=\&quot;canonical\&quot; href=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:url\&quot; content=\&quot;http://localhost:4000/publications/2013-suggero.html\&quot; /&gt;\n&lt;meta property=\&quot;og:site_name\&quot; content=\&quot;CMU Augmented Perception Lab\&quot; /&gt;\n&lt;meta property=\&quot;og:type\&quot; content=\&quot;article\&quot; /&gt;\n&lt;meta property=\&quot;article:published_time\&quot; content=\&quot;2021-04-01T21:20:08-04:00\&quot; /&gt;\n&lt;meta name=\&quot;twitter:card\&quot; content=\&quot;summary\&quot; /&gt;\n&lt;meta property=\&quot;twitter:title\&quot; content=\&quot;Perceptual grouping: selection assistance for digital sketching\&quot; /&gt;\n&lt;script type=\&quot;application/ld+json\&quot;&gt;\n{\&quot;description\&quot;:\&quot;Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These “perceptual groups” are used to suggest possible extensions in response to a person’s initial selection. Two studies were conducted. First, a background study investigated participant’s expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\&quot;,\&quot;mainEntityOfPage\&quot;:{\&quot;@type\&quot;:\&quot;WebPage\&quot;,\&quot;@id\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;},\&quot;headline\&quot;:\&quot;Perceptual grouping: selection assistance for digital sketching\&quot;,\&quot;dateModified\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;url\&quot;:\&quot;http://localhost:4000/publications/2013-suggero.html\&quot;,\&quot;datePublished\&quot;:\&quot;2021-04-01T21:20:08-04:00\&quot;,\&quot;author\&quot;:{\&quot;@type\&quot;:\&quot;Person\&quot;,\&quot;name\&quot;:\&quot;David Lindlbauer\&quot;},\&quot;@type\&quot;:\&quot;BlogPosting\&quot;,\&quot;@context\&quot;:\&quot;https://schema.org\&quot;}&lt;/script&gt;\n&lt;!-- End Jekyll SEO tag --&gt;\n\n&lt;/head&gt;\n\n  &lt;body&gt;\n    &lt;div class=\&quot;content\&quot;&gt;\n      &lt;header class=\&quot;bg-white\&quot;&gt;\n        &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 ph4-l ph3 right pt2 pb4-l pb3 flex items-baseline flex flex-column flex-row-l\&quot;&gt; --&gt;\n        &lt;!-- &lt;div class=\&quot;headermenu w-100 mw8 pa2 flex flex-column\&quot;&gt;           --&gt;\n          &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt4 fw6 link black hover-cmu-red\&quot;&gt;\n          &lt;!-- &lt;a href=\&quot;/\&quot; class=\&quot;dib f2 mt2 fw6 link black absolute right-0 hover-cmu-red w-55 pa3 mr2\&quot;&gt; --&gt;\n            &lt;!-- &lt;picture&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-01-01-01.svg\&quot;\n                type=\&quot;image/svg+xml\&quot;              \n              /&gt; --&gt;\n              &lt;!-- &lt;source\n                srcset=\&quot;/assets/logo-light.webp\&quot;\n                type=\&quot;image/webp\&quot;\n              /&gt; --&gt;\n              &lt;!-- &lt;source  \n                srcset=\&quot;/assets/logo-sphere-03-01.png\&quot;\n                type=\&quot;image/png\&quot;\n              /&gt;\n              &lt;img \n                class=\&quot;logo\&quot;\n                alt=\&quot;A coarsely tesselated sphere colored in shades of gray.\&quot;\n                class=\&quot;pl2 w3\&quot;\n              /&gt; --&gt;              \n            &lt;!-- &lt;/picture&gt; --&gt;\n              Augmented Perception Lab\n          &lt;/a&gt;\n          &lt;!--  --&gt;&lt;nav class=\&quot;mt3 ml-auto-ns lh-copy w-auto-ns w-100 flex flex-column flex-row-ns f4 f5-ns\&quot;&gt;\n            &lt;!-- &lt;nav class=\&quot;mt2 lh-copy w-100 w-25 pa3 mr2\&quot;&gt;&lt;/nav&gt; --&gt;\n            &lt;div class=\&quot;tc mt0-ns mt2\&quot;&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/index\&quot;&gt;Home&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/team\&quot;&gt;Team&lt;/a&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/publications\&quot;&gt;Publications&lt;/a&gt;\n              &lt;!-- &lt;a\n                class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot;\n                href=\&quot;/teaching\&quot;\n                &gt;Teaching&lt;/a\n              &gt; --&gt;\n              &lt;a class=\&quot;ml4-l ml3-m pa0-ns ph2 pv1 b black hover-cmu-red link dib  \&quot; href=\&quot;/contact\&quot;&gt;Contact&lt;/a&gt;\n            &lt;/div&gt;\n          &lt;/nav&gt;\n        &lt;/div&gt;\n\n      &lt;/header&gt;\n\n      &lt;main&gt;\n        &lt;section&gt;\n  &lt;div class=\&quot;pv4 bg-white\&quot;&gt;\n    &lt;div class=\&quot;w-100 mw8 ph4-l ph3 pv2-l pv3 center\&quot;&gt;\n      &lt;h1 class=\&quot;f2 lh-title measure mt3\&quot;&gt;\n        Perceptual grouping: selection assistance for digital sketching\n      &lt;/h1&gt;\n\n      \n\n      &lt;div class=\&quot;mb2\&quot;&gt;\n        \n          &lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;David Lindlbauer&lt;/a&gt;,\n          Michael Haller,\n          Mark Hancock,\n          Stacey D. Scott,\n          Wolfgang Stuerzlinger.\n        &lt;!-- . --&gt;\n        &lt;/div&gt;\n\n      &lt;!-- &lt;div class=\&quot;flex flex-row flex-wrap items-start mt1\&quot;&gt;\n       \n        &lt;div class=\&quot;flex flex-column items-center mt3 mr4\&quot;&gt;\n          &lt;picture&gt;\n            &lt;source srcset=\&quot;/assets/people/david.jpg\&quot;&gt;&lt;/source&gt;\n            &lt;source srcset=\&quot;/assets/person.png\&quot;&gt;&lt;/source&gt;\n            &lt;img class=\&quot;br-100 w3 h3 mb1\&quot; alt=\&quot;Picture of David Lindlbauer\&quot; /&gt;\n          &lt;/picture&gt;\n          &lt;div class=\&quot;black mw4 tc\&quot;&gt;&lt;a href=\&quot;http://davidlindlbauer.com/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;\n  &gt;David Lindlbauer&lt;/a\n&gt;&lt;/div&gt;\n        &lt;/div&gt;\n      \n      &lt;/div&gt; --&gt;\n\n      \n        &lt;div class=\&quot;mt3\&quot;&gt;\n          Published at\n          &lt;span class=\&quot;b\&quot;&gt;\n            \n              &lt;a href=\&quot;https://its2013.acm.org/\&quot; class=\&quot;black underline-dot hover-cmu-red link\&quot;&gt;\n            \n            UIST\n            2013\n            &lt;/a&gt;\n          &lt;/span&gt;\n        &lt;/div&gt;\n      \n\n      \n    &lt;/div&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center mt1\&quot;&gt;\n    \n    &lt;img src=\&quot;/assets/publications/2013-suggero.png\&quot; alt=\&quot;Teaser image\&quot; class=\&quot;mw-100\&quot; style=\&quot;max-height: 600px\&quot;&gt;\n\n    \n\n    \n    \n      &lt;h2&gt;Abstract&lt;/h2&gt;\n      &lt;div class=\&quot;lh-copy\&quot;&gt;\n        Modifying a digital sketch may require multiple selections before a particular editing tool can be applied. Especially on large interactive surfaces, such interactions can be fatiguing. Accordingly, we propose a method, called Suggero, to facilitate the selection process of digital ink. Suggero identifies groups of perceptually related drawing objects. These \&quot;perceptual groups\&quot; are used to suggest possible extensions in response to a person's initial selection. Two studies were conducted. First, a background study investigated participant's expectations of such a selection assistance tool. Then, an empirical study compared the effectiveness of Suggero with an existing manual technique. The results revealed that Suggero required fewer pen interactions and less pen movement, suggesting that Suggero minimizes fatigue during digital sketching.\n\n      &lt;/div&gt;\n    \n\n    &lt;!-- width=\&quot;400\&quot; height=\&quot;240\&quot; class=\&quot;thumb-video\&quot;  --&gt;\n    \n\n    \n    &lt;h2&gt;Materials&lt;/h2&gt;\n      &lt;ul class=\&quot;list pl0\&quot;&gt;\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://dl.acm.org/doi/10.1145/2512349.2512801\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            PDF\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n          &lt;li class=\&quot;mt2\&quot;&gt;&lt;a href=\&quot;https://www.youtube.com/watch?v=oPa7bATFbDc\&quot; class=\&quot;black link hover-cmu-red underline-dot \&quot;&gt;\n            Video: Full length video\n          &lt;/a&gt;&lt;/li&gt;\n        \n\n        \n\n        \n\n        \n\n        \n        \n      &lt;/ul&gt;\n    \n\n    \n      &lt;h2&gt;Bibtex&lt;/h2&gt;\n      &lt;div class=\&quot;f6 underline-dot\&quot;&gt;\n        &lt;pre&gt;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&lt;/pre&gt;\n      &lt;/div&gt;\n    \n    \n  &lt;/div&gt;\n&lt;/section&gt;\n\n      &lt;/main&gt;\n    &lt;/div&gt;\n\n    &lt;footer class=\&quot;bt white mt5 flex-shrink-0\&quot;&gt;\n      &lt;div class=\&quot;w-100 mw8 ph4-l ph3 center pv3 footerinfo\&quot;&gt;\n        &lt;ul class=\&quot;list pl0\&quot;&gt;\n          &lt;li&gt;\n            &lt;a href=\&quot;/index\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Augmented Perception Lab&lt;/a&gt;, \n            &lt;a href=\&quot;https://hcii.cmu.edu\&quot; class=\&quot;link white underline-dot-white hover-cmu-red mv1\&quot;&gt;Human-Computer Interaction Institute&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cs.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;School of Computer Science&lt;/a&gt;,\n            &lt;a href=\&quot;https://www.cmu.edu\&quot; class=\&quot;link white dib underline-dot-white hover-cmu-red mv1\&quot;&gt;Carnegie Mellon University&lt;/a&gt;\n            &lt;!-- &lt;span &gt;&lt;/span&gt;  --&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;span&gt;\n            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, &lt;a href=\&quot;/contact\&quot; class=\&quot;link white dim underline-dot-white hover-cmu-red\&quot;&gt;How to find us.&lt;/a&gt;\n            &lt;/span&gt;\n          &lt;/li&gt;\n          &lt;li class=\&quot;pv1\&quot;&gt;\n            &lt;abbr title=\&quot;Last build on 2021-04-01\&quot; class=\&quot;white\&quot; style=\&quot;list-style: height 2em; text-decoration: none;\&quot;&gt;Last update April 2021&lt;/abbr&gt;              \n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/div&gt;\n    &lt;/footer&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n&quot;,&quot;url&quot;:&quot;/publications/2013-suggero.html&quot;,&quot;id&quot;:&quot;/publications/2013-suggero&quot;,&quot;relative_path&quot;:&quot;_publications/2013-suggero.html&quot;,&quot;draft&quot;:false,&quot;categories&quot;:[],&quot;layout&quot;:&quot;publication&quot;,&quot;year&quot;:2013,&quot;month&quot;:10,&quot;selected&quot;:false,&quot;link&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;pdf&quot;:&quot;https://dl.acm.org/doi/10.1145/2512349.2512801&quot;,&quot;title&quot;:&quot;Perceptual grouping: selection assistance for digital sketching&quot;,&quot;authors&quot;:[&quot;David Lindlbauer&quot;,&quot;Michael Haller&quot;,&quot;Mark Hancock&quot;,&quot;Stacey D. Scott&quot;,&quot;Wolfgang Stuerzlinger&quot;],&quot;doi&quot;:&quot;10.1145/2512349.2512801&quot;,&quot;venue_location&quot;:&quot;St. Andrews, Scotland&quot;,&quot;venue_url&quot;:&quot;https://its2013.acm.org/&quot;,&quot;venue_tags&quot;:[&quot;ITS&quot;],&quot;type&quot;:[&quot;Conference&quot;],&quot;tags&quot;:[&quot;Science&quot;,&quot;Sketching&quot;,&quot;Large displays&quot;],&quot;venue&quot;:&quot;UIST&quot;,&quot;video-thumb&quot;:&quot;oPa7bATFbDc&quot;,&quot;video-suppl&quot;:&quot;oPa7bATFbDc&quot;,&quot;bibtex&quot;:&quot;@inproceedings{10.1145/2512349.2512801,\n author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},\n title = {Perceptual Grouping: Selection Assistance for Digital Sketching},\n year = {2013},\n isbn = {9781450322713},\n publisher = {Association for Computing Machinery},\n address = {New York, NY, USA},\n url = {https://doi.org/10.1145/2512349.2512801},\n doi = {10.1145/2512349.2512801},\n booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},\n pages = {51–60},\n numpages = {10},\n keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},\n location = {St. Andrews, Scotland, United Kingdom},\n series = {ITS '13}\n }&quot;,&quot;slug&quot;:&quot;2013-suggero&quot;,&quot;ext&quot;:&quot;.html&quot;,&quot;date&quot;:&quot;2021-04-01 21:20:08 -0400&quot;}">
            
            
              <div class="h3 mv3 mr3-ns mb2 pa0 mb0-ns flex-shrink-0 preview-image ba b--black-05 dn db-ns " style="background-image: url('/assets/publications/2013-suggero_thumb.png')">   
              
              <iframe width="100%" height="100%" class="thumb-video" src="https://www.youtube.com/embed/oPa7bATFbDc?iv_load_policy=3&amp;modestbranding=1&amp;rel=0&amp;autohide=1&amp;playsinline=1&amp;controls=1&amp;showinfo=0&amp;autoplay=0&amp;loop=0&amp;mute=1" frameborder="0" allowfullscreen></iframe>   
              
              </div>

            <div class="measure-wide mv3 min-width-front">
              <h3 class="mt0 f4 measure-wide mb2" id="/publications/2013-suggero">
                  <a href="/publications/2013-suggero.html" class="black hover-cmu-red link">Perceptual grouping: selection assistance for digital sketching</a>
</h3>
              
                  <div class="mb2">
                
                  <a href="http://davidlindlbauer.com/" class="black underline-dot hover-cmu-red link">David Lindlbauer</a>, 
                  Michael Haller, 
                  Mark Hancock, 
                  Stacey D. Scott, 
                  Wolfgang Stuerzlinger.
              </div>


              <div class="mt3">
              Published at
                <span class="b">
                
                <a href="" class="black underline-dot hover-cmu-red link">
                
                UIST
                2013
                </a>
                </span>
              </div>

              

              

              

              <div class="mt2 mb1">
                <a href="/publications/2013-suggero.html" class="mr3 dib">
                  <span class="cta">Show Details</span>
                </a>

                
                <a href="https://dl.acm.org/doi/10.1145/2512349.2512801" class="black link underline-dot hover-cmu-red mr3 dib">PDF</a>
                

                
                <a href="https://doi.org/10.1145/2512349.2512801" class="black link underline-dot hover-cmu-red mr3 dib">
                  DOI
                </a>
                

                
                                
                
                <a href="oPa7bATFbDc" class="black link underline-dot hover-cmu-red mr3 dib">
                  Full video
                </a>
                
                
                <!-- 
                 -->

                
                <label for="slide_perceptual-grouping-selection-assistance-for-digital-sketching" class="accordion__item-label db pv3 link black hover-cmu-red mr3 dib"> Bibtex</label>
                <div class="accordion__item">
                  <input type="checkbox" class="dn" name="slides" id="slide_perceptual-grouping-selection-assistance-for-digital-sketching">
                  <div class="accordion__content bb b--black-20 f6">
                    <pre>@inproceedings{10.1145/2512349.2512801,
 author = {Lindlbauer, David and Haller, Michael and Hancock, Mark and Scott, Stacey D. and Stuerzlinger, Wolfgang},
 title = {Perceptual Grouping: Selection Assistance for Digital Sketching},
 year = {2013},
 isbn = {9781450322713},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/2512349.2512801},
 doi = {10.1145/2512349.2512801},
 booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},
 pages = {51–60},
 numpages = {10},
 keywords = {perceptual grouping, selection assistance, digital sketching, sketch analysis, interactive whiteboard},
 location = {St. Andrews, Scotland, United Kingdom},
 series = {ITS '13}
 }</pre>
                  </div>
                </div>
                

                <!-- 
                

                

                 
                -->
              </div>
            </div>
      </div>
    
  
</article>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>

<script>
  (function(e){if("object"==typeof exports&&"undefined"!=typeof module)module.exports=e();else if("function"==typeof define&&define.amd)define([],e);else{var t;t="undefined"==typeof window?"undefined"==typeof global?"undefined"==typeof self?this:self:global:window,t.itemsjs=e()}})(function(){var W,xn=Math.max;return function(){function s(l,e,n){function t(d,r){if(!e[d]){if(!l[d]){var i="function"==typeof require&&require;if(!r&&i)return i(d,!0);if(o)return o(d,!0);var c=new Error("Cannot find module '"+d+"'");throw c.code="MODULE_NOT_FOUND",c}var a=e[d]={exports:{}};l[d][0].call(a.exports,function(e){var o=l[d][1][e];return t(o||e)},a,a.exports,s,l,e,n)}return e[d].exports}for(var o="function"==typeof require&&require,r=0;r<n.length;r++)t(n[r]);return t}return s}()({1:[function(e,t){"use strict";t.exports=e("./src/index")},{"./src/index":5}],2:[function(e,t,n){(function(){var e=Math.log,o=Math.floor,r=function(e){var t=new r.Index;return t.pipeline.add(r.trimmer,r.stopWordFilter,r.stemmer),e&&e.call(t,t),t};r.version="1.0.0",r.utils={},r.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),r.utils.asString=function(e){return void 0===e||null===e?"":e.toString()},r.EventEmitter=function(){this.events={}},r.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop();if("function"!=typeof t)throw new TypeError("last argument must be a function");e.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},r.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);this.events[e].splice(n,1),this.events[e].length||delete this.events[e]}},r.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)})}},r.EventEmitter.prototype.hasHandler=function(e){return e in this.events},r.tokenizer=function(e){return arguments.length&&null!=e&&void 0!=e?Array.isArray(e)?e.map(function(e){return r.utils.asString(e).toLowerCase()}):e.toString().trim().toLowerCase().split(r.tokenizer.separator):[]},r.tokenizer.separator=/[\s\-]+/,r.tokenizer.load=function(e){var t=this.registeredFunctions[e];if(!t)throw new Error("Cannot load un-registered function: "+e);return t},r.tokenizer.label="default",r.tokenizer.registeredFunctions={default:r.tokenizer},r.tokenizer.registerFunction=function(e,t){t in this.registeredFunctions&&r.utils.warn("Overwriting existing tokenizer: "+t),e.label=t,this.registeredFunctions[t]=e},r.Pipeline=function(){this._stack=[]},r.Pipeline.registeredFunctions={},r.Pipeline.registerFunction=function(e,t){t in this.registeredFunctions&&r.utils.warn("Overwriting existing registered function: "+t),e.label=t,r.Pipeline.registeredFunctions[e.label]=e},r.Pipeline.warnIfFunctionNotRegistered=function(e){var t=e.label&&e.label in this.registeredFunctions;t||r.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},r.Pipeline.load=function(e){var t=new r.Pipeline;return e.forEach(function(e){var n=r.Pipeline.registeredFunctions[e];if(n)t.add(n);else throw new Error("Cannot load un-registered function: "+e)}),t},r.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){r.Pipeline.warnIfFunctionNotRegistered(e),this._stack.push(e)},this)},r.Pipeline.prototype.after=function(e,t){r.Pipeline.warnIfFunctionNotRegistered(t);var n=this._stack.indexOf(e);if(-1==n)throw new Error("Cannot find existingFn");++n,this._stack.splice(n,0,t)},r.Pipeline.prototype.before=function(e,t){r.Pipeline.warnIfFunctionNotRegistered(t);var n=this._stack.indexOf(e);if(-1==n)throw new Error("Cannot find existingFn");this._stack.splice(n,0,t)},r.Pipeline.prototype.remove=function(e){var t=this._stack.indexOf(e);-1==t||this._stack.splice(t,1)},r.Pipeline.prototype.run=function(e){for(var t,n=[],o=e.length,r=this._stack.length,s=0;s<o;s++){t=e[s];for(var i=0;i<r&&(t=this._stack[i](t,s,e),void 0!==t&&""!==t);i++);void 0!==t&&""!==t&&n.push(t)}return n},r.Pipeline.prototype.reset=function(){this._stack=[]},r.Pipeline.prototype.toJSON=function(){return this._stack.map(function(e){return r.Pipeline.warnIfFunctionNotRegistered(e),e.label})},r.Vector=function(){this._magnitude=null,this.list=void 0,this.length=0},r.Vector.Node=function(e,t,n){this.idx=e,this.val=t,this.next=n},r.Vector.prototype.insert=function(e,t){this._magnitude=void 0;var n=this.list;if(!n)return this.list=new r.Vector.Node(e,t,n),this.length++;if(e<n.idx)return this.list=new r.Vector.Node(e,t,n),this.length++;for(var o=n,i=n.next;void 0!=i;){if(e<i.idx)return o.next=new r.Vector.Node(e,t,i),this.length++;o=i,i=i.next}return o.next=new r.Vector.Node(e,t,i),this.length++},r.Vector.prototype.magnitude=function(){if(this._magnitude)return this._magnitude;for(var e,t=this.list,n=0;t;)e=t.val,n+=e*e,t=t.next;return this._magnitude=Math.sqrt(n)},r.Vector.prototype.dot=function(e){for(var t=this.list,n=e.list,o=0;t&&n;)t.idx<n.idx?t=t.next:t.idx>n.idx?n=n.next:(o+=t.val*n.val,t=t.next,n=n.next);return o},r.Vector.prototype.similarity=function(e){return this.dot(e)/(this.magnitude()*e.magnitude())},r.SortedSet=function(){this.length=0,this.elements=[]},r.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},r.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},r.SortedSet.prototype.toArray=function(){return this.elements.slice()},r.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},r.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},r.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,r=n-t,i=t+o(r/2),s=this.elements[i];1<r;){if(s===e)return i;s<e&&(t=i),s>e&&(n=i),r=n-t,i=t+o(r/2),s=this.elements[i]}return s===e?i:-1},r.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,r=n-t,i=t+o(r/2),s=this.elements[i];1<r;)s<e&&(t=i),s>e&&(n=i),r=n-t,i=t+o(r/2),s=this.elements[i];return s>e?i:s<e?i+1:void 0},r.SortedSet.prototype.intersect=function(e){for(var t=new r.SortedSet,n=0,o=0,i=this.length,s=e.length,l=this.elements,a=e.elements;!0&&!(n>i-1||o>s-1);){if(l[n]===a[o]){t.add(l[n]),n++,o++;continue}if(l[n]<a[o]){n++;continue}if(l[n]>a[o]){o++;continue}}return t},r.SortedSet.prototype.clone=function(){var e=new r.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},r.SortedSet.prototype.union=function(e){var t,n,o;this.length>=e.length?(t=this,n=e):(t=e,n=this),o=t.clone();for(var r=0,i=n.toArray();r<i.length;r++)o.add(i[r]);return o},r.SortedSet.prototype.toJSON=function(){return this.toArray()},r.Index=function(){this._fields=[],this._ref="id",this.pipeline=new r.Pipeline,this.documentStore=new r.Store,this.tokenStore=new r.TokenStore,this.corpusTokens=new r.SortedSet,this.eventEmitter=new r.EventEmitter,this.tokenizerFn=r.tokenizer,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},r.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},r.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},r.Index.load=function(e){e.version!==r.version&&r.utils.warn("version mismatch: current "+r.version+" importing "+e.version);var t=new this;return t._fields=e.fields,t._ref=e.ref,t.tokenizer(r.tokenizer.load(e.tokenizer)),t.documentStore=r.Store.load(e.documentStore),t.tokenStore=r.TokenStore.load(e.tokenStore),t.corpusTokens=r.SortedSet.load(e.corpusTokens),t.pipeline=r.Pipeline.load(e.pipeline),t},r.Index.prototype.field=function(e,t){var t=t||{},n={name:e,boost:t.boost||1};return this._fields.push(n),this},r.Index.prototype.ref=function(e){return this._ref=e,this},r.Index.prototype.tokenizer=function(e){var t=e.label&&e.label in r.tokenizer.registeredFunctions;return t||r.utils.warn("Function is not a registered tokenizer. This may cause problems when serialising the index"),this.tokenizerFn=e,this},r.Index.prototype.add=function(e,t){var n={},o=new r.SortedSet,s=e[this._ref],t=!(t!==void 0)||t;this._fields.forEach(function(t){var r=this.pipeline.run(this.tokenizerFn(e[t.name]));n[t.name]=r;for(var s,a=0;a<r.length;a++)s=r[a],o.add(s),this.corpusTokens.add(s)},this),this.documentStore.set(s,o);for(var a=0;a<o.length;a++){for(var i=o.elements[a],l=0,d=0;d<this._fields.length;d++){var c=this._fields[d],p=n[c.name],u=p.length;if(u){for(var g=0,f=0;f<u;f++)p[f]===i&&g++;l+=g/u*c.boost}}this.tokenStore.add(i,{ref:s,tf:l})}t&&this.eventEmitter.emit("add",e,this)},r.Index.prototype.remove=function(e,t){var n=e[this._ref],t=!(t!==void 0)||t;if(this.documentStore.has(n)){var o=this.documentStore.get(n);this.documentStore.remove(n),o.forEach(function(e){this.tokenStore.remove(e,n)},this),t&&this.eventEmitter.emit("remove",e,this)}},r.Index.prototype.update=function(e,t){var t=!(t!==void 0)||t;this.remove(e,!1),this.add(e,!1),t&&this.eventEmitter.emit("update",e,this)},r.Index.prototype.idf=function(t){var n="@"+t;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var o=this.tokenStore.count(t),r=1;return 0<o&&(r=1+e(this.documentStore.length/o)),this._idfCache[n]=r},r.Index.prototype.search=function(t){var n=this.pipeline.run(this.tokenizerFn(t)),o=new r.Vector,s=[],a=this._fields.reduce(function(e,t){return e+t.boost},0),i=n.some(function(e){return this.tokenStore.has(e)},this);if(!i)return[];n.forEach(function(t,n,i){var l=1/i.length*this._fields.length*a,d=this,c=this.tokenStore.expand(t).reduce(function(n,s){var a=d.corpusTokens.indexOf(s),c=d.idf(s),p=1,u=new r.SortedSet;if(s!==t){var g=xn(3,s.length-t.length);p=1/e(g)}-1<a&&o.insert(a,l*c*p);for(var f=d.tokenStore.get(s),h=Object.keys(f),y=h.length,_=0;_<y;_++)u.add(f[h[_]].ref);return n.union(u)},new r.SortedSet);s.push(c)},this);var l=s.reduce(function(e,t){return e.intersect(t)});return l.map(function(e){return{ref:e,score:o.similarity(this.documentVector(e))}},this).sort(function(e,t){return t.score-e.score})},r.Index.prototype.documentVector=function(e){for(var t=this.documentStore.get(e),n=t.length,o=new r.Vector,s=0;s<n;s++){var i=t.elements[s],a=this.tokenStore.get(i)[e].tf,l=this.idf(i);o.insert(this.corpusTokens.indexOf(i),a*l)}return o},r.Index.prototype.toJSON=function(){return{version:r.version,fields:this._fields,ref:this._ref,tokenizer:this.tokenizerFn.label,documentStore:this.documentStore.toJSON(),tokenStore:this.tokenStore.toJSON(),corpusTokens:this.corpusTokens.toJSON(),pipeline:this.pipeline.toJSON()}},r.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},r.Store=function(){this.store={},this.length=0},r.Store.load=function(e){var t=new this;return t.length=e.length,t.store=Object.keys(e.store).reduce(function(t,n){return t[n]=r.SortedSet.load(e.store[n]),t},{}),t},r.Store.prototype.set=function(e,t){this.has(e)||this.length++,this.store[e]=t},r.Store.prototype.get=function(e){return this.store[e]},r.Store.prototype.has=function(e){return e in this.store},r.Store.prototype.remove=function(e){this.has(e)&&(delete this.store[e],this.length--)},r.Store.prototype.toJSON=function(){return{store:this.store,length:this.length}},r.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[aeiouy]",o="[^aeiou]"+"[^aeiouy]*",r=n+"[aeiou]*",i=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*/,s=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*[aeiouy][aeiou]*[^aeiou][^aeiouy]*/,a=/^([^aeiou][^aeiouy]*)?[aeiouy][aeiou]*[^aeiou][^aeiouy]*([aeiouy][aeiou]*)?$/,l=/^([^aeiou][^aeiouy]*)?[aeiouy]/,d=/^(.+?)(ss|i)es$/,c=/^(.+?)([^s])s$/,p=/^(.+?)eed$/,u=/^(.+?)(ed|ing)$/,g=/.$/,f=/(at|bl|iz)$/,h=/([^aeiouylsz])\1$/,y=/^[^aeiou][^aeiouy]*[aeiouy][^aeiouwxy]$/,_=/^(.+?[^aeiou])y$/,m=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,b=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,j=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,k=/^(.+?)(s|t)(ion)$/,x=/^(.+?)e$/,S=/ll$/,v=/^[^aeiou][^aeiouy]*[aeiouy][^aeiouwxy]$/;return function(n){var o,r,A,F,w,z,O;if(3>n.length)return n;if(A=n.substr(0,1),"y"==A&&(n=A.toUpperCase()+n.substr(1)),F=d,w=c,F.test(n)?n=n.replace(F,"$1$2"):w.test(n)&&(n=n.replace(w,"$1$2")),F=p,w=u,F.test(n)){var E=F.exec(n);F=i,F.test(E[1])&&(F=g,n=n.replace(F,""))}else if(w.test(n)){var E=w.exec(n);o=E[1],w=l,w.test(o)&&(n=o,w=f,z=h,O=y,w.test(n)?n+="e":z.test(n)?(F=g,n=n.replace(F,"")):O.test(n)&&(n+="e"))}if(F=_,F.test(n)){var E=F.exec(n);o=E[1],n=o+"i"}if(F=m,F.test(n)){var E=F.exec(n);o=E[1],r=E[2],F=i,F.test(o)&&(n=o+e[r])}if(F=b,F.test(n)){var E=F.exec(n);o=E[1],r=E[2],F=i,F.test(o)&&(n=o+t[r])}if(F=j,w=k,F.test(n)){var E=F.exec(n);o=E[1],F=s,F.test(o)&&(n=o)}else if(w.test(n)){var E=w.exec(n);o=E[1]+E[2],w=s,w.test(o)&&(n=o)}if(F=x,F.test(n)){var E=F.exec(n);o=E[1],F=s,w=a,z=v,(F.test(o)||w.test(o)&&!z.test(o))&&(n=o)}return F=S,w=s,F.test(n)&&w.test(n)&&(F=g,n=n.replace(F,"")),"y"==A&&(n=A.toLowerCase()+n.substr(1)),n}}(),r.Pipeline.registerFunction(r.stemmer,"stemmer"),r.generateStopWordFilter=function(e){var t=e.reduce(function(e,t){return e[t]=t,e},{});return function(e){if(e&&t[e]!==e)return e}},r.stopWordFilter=r.generateStopWordFilter(["a","able","about","across","after","all","almost","also","am","among","an","and","any","are","as","at","be","because","been","but","by","can","cannot","could","dear","did","do","does","either","else","ever","every","for","from","get","got","had","has","have","he","her","hers","him","his","how","however","i","if","in","into","is","it","its","just","least","let","like","likely","may","me","might","most","must","my","neither","no","nor","not","of","off","often","on","only","or","other","our","own","rather","said","say","says","she","should","since","so","some","than","that","the","their","them","then","there","these","they","this","tis","to","too","twas","us","wants","was","we","were","what","when","where","which","while","who","whom","why","will","with","would","yet","you","your"]),r.Pipeline.registerFunction(r.stopWordFilter,"stopWordFilter"),r.trimmer=function(e){return e.replace(/^\W+/,"").replace(/\W+$/,"")},r.Pipeline.registerFunction(r.trimmer,"trimmer"),r.TokenStore=function(){this.root={docs:{}},this.length=0},r.TokenStore.load=function(e){var t=new this;return t.root=e.root,t.length=e.length,t},r.TokenStore.prototype.add=function(e,t,n){var n=n||this.root,o=e.charAt(0),r=e.slice(1);return o in n||(n[o]={docs:{}}),0===r.length?(n[o].docs[t.ref]=t,void(this.length+=1)):this.add(r,t,n[o])},r.TokenStore.prototype.has=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e.charAt(n)])return!1;t=t[e.charAt(n)]}return!0},r.TokenStore.prototype.getNode=function(e){if(!e)return{};for(var t=this.root,n=0;n<e.length;n++){if(!t[e.charAt(n)])return{};t=t[e.charAt(n)]}return t},r.TokenStore.prototype.get=function(e,t){return this.getNode(e,t).docs||{}},r.TokenStore.prototype.count=function(e,t){return Object.keys(this.get(e,t)).length},r.TokenStore.prototype.remove=function(e,t){if(e){for(var n=this.root,o=0;o<e.length;o++){if(!(e.charAt(o)in n))return;n=n[e.charAt(o)]}delete n.docs[t]}},r.TokenStore.prototype.expand=function(e,t){var n=this.getNode(e),o=n.docs||{},t=t||[];return Object.keys(o).length&&t.push(e),Object.keys(n).forEach(function(n){"docs"===n||t.concat(this.expand(e+n,t))},this),t},r.TokenStore.prototype.toJSON=function(){return{root:this.root,length:this.length}},function(e,o){"function"==typeof W&&W.amd?W(o):"object"==typeof n?t.exports=o():e.lunr=o()}(this,function(){return r})})()},{}],3:[function(e,t){"use strict";var n=e("./../vendor/lodash"),o=e("lunr"),r=function(e,t){var r=this;t=t||{},t.searchableFields=t.searchableFields||[],this.items=e,this.idx=o(function(){this.field("name",{boost:10});var e=this;n.forEach(t.searchableFields,function(t){e.field(t)}),this.ref("id"),t.isExactSearch&&(this.pipeline.remove(o.stemmer),this.pipeline.remove(o.stopWordFilter))});var s=1;n.map(e,function(e){e.id||(e.id=s,++s),r.idx.add(e)}),this.store=n.mapKeys(e,function(e){return e.id})};r.prototype={search:function(e){var t=this;return e?n.map(this.idx.search(e),function(e){var n=t.store[e.ref];return n}):this.items}},t.exports=r},{"./../vendor/lodash":7,lunr:2}],4:[function(e,t){"use strict";var n=e("./../vendor/lodash");t.exports.includes=function(e,t){return!t||n.every(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.includes_any=function(e,t){return!t||t instanceof Array&&0===t.length||n.some(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.includes_any_element=function(e,t){return n.some(t,function(t){return"string"==typeof e||e instanceof String?t===e:n.includes(e,t)})},t.exports.intersection=function(e,t){return t?n.intersection(e,n.flatten(t)):e};var o=function(e){try{return JSON.parse(JSON.stringify(e))}catch(t){return e}};t.exports.mergeAggregations=function(e,t){return n.mapValues(o(e),function(e,n){e.field||(e.field=n);var o=[];t.filters&&t.filters[n]&&(o=t.filters[n]),e.filters=o;var r=[];return t.not_filters&&t.not_filters[n]&&(r=t.not_filters[n]),t.exclude_filters&&t.exclude_filters[n]&&(r=t.exclude_filters[n]),e.not_filters=r,e})};t.exports.is_conjunctive_agg=function(e){return!1!==e.conjunction},t.exports.is_disjunctive_agg=function(e){return!1===e.conjunction},t.exports.is_not_filters_agg=function(e){return e.not_filters instanceof Array&&0<e.not_filters.length},t.exports.is_empty_agg=function(e){return"is_empty"===e.type},t.exports.conjunctive_field=function(e,n){return t.exports.includes(e,n)},t.exports.disjunctive_field=function(e,n){return t.exports.includes_any(e,n)},t.exports.not_filters_field=function(e,n){return!t.exports.includes_any_element(e,n)},t.exports.check_empty_field=function(e,n){var o=["not_empty"];return(""===e||void 0===e||null===e||e instanceof Array&&0===e.length)&&(o=["empty"]),n&&!t.exports.includes(o,n)?!1:o}},{"./../vendor/lodash":7}],5:[function(e,t){"use strict";var n=e("./lib"),o=e("./../vendor/lodash"),r=e("./helpers"),i=e("./fulltext");t.exports=function(e,t){t=t||{};var o=new i(e,t);return{search:function(i){return i=i||{},i.aggregations=r.mergeAggregations(t.aggregations,i),n.search(e,i,t,o)},similar:function(t,o){return n.similar(e,t,o)},aggregation:function(o){return n.aggregation(e,o,t.aggregations)},reindex:function(n){e=n,o=new i(e,t)}}}},{"./../vendor/lodash":7,"./fulltext":3,"./helpers":4,"./lib":6}],6:[function(e,t){"use strict";var n=e("./../vendor/lodash"),o=e("./helpers"),r=e("./fulltext");t.exports.search=function(e,n,o,r){n=n||{};var i=0;if(r){var s=new Date().getTime();e=r.search(n.query),i=new Date().getTime()-s}n.filter instanceof Function&&(e=e.filter(n.filter)),n.prefilter instanceof Function&&(e=n.prefilter(e));var a=t.exports.items_by_aggregations(e,n.aggregations),l=n.per_page||12,d=n.page||1,c=0;if(n.sort){var p=new Date().getTime();a=t.exports.sorted_items(a,n.sort,o.sortings),c=new Date().getTime()-p}var u=new Date().getTime(),g=t.exports.aggregations(e,n.aggregations),f=new Date().getTime()-u;return{pagination:{per_page:l,page:d,total:a.length},timings:{facets:f,search:i,sorting:c},data:{items:a.slice((d-1)*l,d*l),aggregations:g}}},t.exports.aggregation=function(e,o,r){var i=o.per_page||10,s=o.page||1;if(o.name&&(!r||!r[o.name]))throw new Error("Please define aggregation \"".concat(o.name,"\" in config"));var a=t.exports.buckets(e,o.name,r[o.name],r);return o.query&&(a=n.filter(a,function(e){return 0===e.key.toLowerCase().indexOf(o.query.toLowerCase())})),{pagination:{per_page:i,page:s,total:a.length},data:{buckets:a.slice((s-1)*i,s*i)}}},t.exports.sorted_items=function(e,t,o){return o&&o[t]&&(t=o[t]),t.field?n.orderBy(e,t.field,t.order||"asc"):e},t.exports.items_by_aggregations=function(e,o){return n.filter(e,function(e){return t.exports.filterable_item(e,o)})},t.exports.aggregations=function(e,o){var r=0;return n.mapValues(o,function(n,i){return++r,{name:i,title:n.title||i.charAt(0).toUpperCase()+i.slice(1),position:r,buckets:t.exports.buckets(e,i,n,o).slice(0,n.size||10)}})},t.exports.filterable_item=function(e,t){for(var r,s=n.keys(t),a=0;a<s.length;++a){if(r=s[a],o.is_empty_agg(t[r])){if(o.check_empty_field(e[t[r].field],t[r].filters))continue;return!1}if(o.is_not_filters_agg(t[r])&&!o.not_filters_field(e[r],t[r].not_filters))return!1;if(o.is_disjunctive_agg(t[r])&&!o.disjunctive_field(e[r],t[r].filters))return!1;if(o.is_conjunctive_agg(t[r])&&!o.conjunctive_field(e[r],t[r].filters))return!1}return!0},t.exports.bucket_field=function(e,t,r){for(var s,a=n.keys(t),l=0;l<a.length;++l)if(s=a[l],o.is_not_filters_agg(t[s])&&!o.not_filters_field(e[s],t[s].not_filters))return[];for(var l=0;l<a.length;++l)if(a[l]!==r){var s=a[l];if(o.is_empty_agg(t[s])){if(!o.check_empty_field(e[t[s].field],t[s].filters))return[];continue}else{if(o.is_disjunctive_agg(t[s])&&!o.disjunctive_field(e[s],t[s].filters))return[];if(o.is_conjunctive_agg(t[s])&&!o.conjunctive_field(e[s],t[s].filters))return[]}}if(o.is_empty_agg(t[r])){var i=o.check_empty_field(e[t[r].field],t[r].filters);return i?i:[]}return o.is_disjunctive_agg(t[r])||o.includes(e[r],t[r].filters)?e[r]?n.flatten([e[r]]):[]:[]},t.exports.bucket=function(e,o){return n.mapValues(o,function(n,r){return t.exports.bucket_field(e,o,r)})},t.exports.buckets=function(e,r,s,a){var i=n.transform(e,function(e,n){n=t.exports.bucket(n,a);var l=n[r];if(!1!==s.conjunction&&o.includes(l,s.filters)||!1===s.conjunction)for(var d,c=0;l&&c<l.length;++c)d=l[c],e[d]?e[d]+=1:e[d]=1},{});return i=n.map(i,function(e,t){return{key:t,doc_count:e,selected:n.includes(s.filters,t)}}),i="term"===s.sort?n.orderBy(i,["selected","key"],["desc",s.order||"asc"]):n.orderBy(i,["selected","doc_count","key"],["desc",s.order||"desc","asc"]),i},t.exports.similar=function(e,t,o){for(var r,s=o.per_page||10,a=o.minimum||0,l=o.page||1,d=0;d<e.length;++d)if(e[d].id==t){r=e[d];break}if(!o.field)throw new Error("Please define field in options");for(var i=o.field,c=[],d=0;d<e.length;++d)if(e[d].id!==t){var p=n.intersection(r[i],e[d][i]);p.length>=a&&(c.push(e[d]),c[c.length-1].intersection_length=p.length)}return c=n.orderBy(c,["intersection_length"],["desc"]),{pagination:{per_page:s,page:l,total:c.length},data:{items:c.slice((l-1)*s,l*s)}}}},{"./../vendor/lodash":7,"./fulltext":3,"./helpers":4}],7:[function(e,Fn,zn){(function(En){"use strict";function Tn(e){return Tn="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e},Tn(e)}(function(){function In(n,o){return n.set(o[0],o[1]),n}function t(n,t){return n.add(t),n}function Pn(o,t,e){switch(e.length){case 0:return o.call(t);case 1:return o.call(t,e[0]);case 2:return o.call(t,e[0],e[1]);case 3:return o.call(t,e[0],e[1],e[2]);}return o.apply(t,e)}function Nn(o,t){for(var e=-1,n=null==o?0:o.length;++e<n&&!1!==t(o[e],e,o););return o}function Vn(o,t){for(var e=-1,n=null==o?0:o.length;++e<n;)if(!t(o[e],e,o))return!1;return!0}function n(i,t){for(var e,s=-1,n=null==i?0:i.length,r=0,o=[];++s<n;)e=i[s],t(e,s,i)&&(o[r++]=e);return o}function u(n,t){return null!=n&&n.length&&-1<(t===t?y(n,t,0):o(n,s,0))}function c(i,t){for(var e=-1,n=null==i?0:i.length,r=Array(n);++e<n;)r[e]=t(i[e],e,i);return r}function Cn(i,t){for(var e=-1,n=t.length,r=i.length;++e<n;)i[r+e]=t[e];return i}function e(i,t,e){for(var n=-1,r=null==i?0:i.length;++n<r;)e=t(e,i[n],n,i);return e}function i(o,t){for(var e=-1,n=null==o?0:o.length;++e<n;)if(t(o[e],e,o))return!0;return!1}function o(o,t,e){var n=o.length;for(e+=-1;++e<n;)if(t(o[e],e,o))return e;return-1}function s(e){return e!==e}function r(n){return function(t){return null==t?Kt:t[n]}}function a(o,t){var e=o.length;for(o.sort(t);e--;)o[e]=o[e].c;return o}function l(n){return function(t){return n(t)}}function p(n,t){return c(t,function(t){return n[t]})}function f(o){var i=-1,e=Array(o.size);return o.forEach(function(n,t){e[++i]=[t,n]}),e}function h(o){var t=Object;return function(e){return o(t(e))}}function g(o){var r=-1,e=Array(o.size);return o.forEach(function(n){e[++r]=n}),e}function y(o,t,e){--e;for(var n=o.length;++e<n;)if(o[e]===t)return e;return-1}function d(){}function _(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function m(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function b(o){var t=-1,e=null==o?0:o.length;for(this.clear();++t<e;){var n=o[t];this.set(n[0],n[1])}}function j(o){var t=-1,e=null==o?0:o.length;for(this.__data__=new b;++t<e;)this.add(o[t])}function S(e){this.size=(this.__data__=new m(e)).size}function k(s,t){var l=vn(s),d=!l&&mn(s),p=!l&&!d&&An(s),g=!l&&!d&&!p&&Sn(s);if(l=l||d||p||g){for(var d=s.length,u=String,c=-1,i=Array(d);++c<d;)i[c]=u(c);d=i}else d=[];var a,u=d.length;for(a in s)!t&&!Fe.call(s,a)||l&&("length"==a||p&&("offset"==a||"parent"==a)||g&&("buffer"==a||"byteLength"==a||"byteOffset"==a)||gt(a,u))||d.push(a);return d}function v(o,t,e){var n=o[t];Fe.call(o,t)&&xt(n,e)&&(e!==Kt||t in o)||w(o,t,e)}function x(o,t){for(var e=o.length;e--;)if(xt(o[e][0],t))return e;return-1}function A(n,t){return n&&et(t,Tt(t),n)}function F(n,t){return n&&et(t,Mt(t),n)}function w(o,t,e){"__proto__"==t&&Te?Te(o,t,{configurable:!0,enumerable:!0,value:e,writable:!0}):o[t]=e}function z(l,t,e,n,o,d){var c,r=1&t,i=2&t;if(e&&(c=o?e(l,n,o,d):e(l)),c!==Kt)return c;if(!Ot(l))return l;if(!(n=vn(l))){var p=bn(l),s="[object Function]"==p||"[object GeneratorFunction]"==p;if(An(l))return X(l,r);if("[object Object]"!=p&&"[object Arguments]"!=p&&(!s||o)){if(!pe[p])return o?l:{};c=ft(l,p,z,r)}else if(c=i||s?{}:"function"!=typeof l.constructor||_t(l)?{}:sn(Le(l)),!r)return i?nt(l,F(c,l)):tt(l,A(c,l))}else if(c=ut(l),!r)return Z(l,c);if(d||(d=new S),o=d.get(l))return o;d.set(l,c);var i=4&t?i?at:it:i?Mt:Tt,u=n?Kt:i(l);return Nn(u||l,function(n,r){u&&(r=n,n=l[r]),v(c,r,z(n,t,e,r,l,d))}),c}function O(o,i){var e=!0;return un(o,function(n,t,r){return e=!!i(n,t,r)}),e}function E(o,i){var e=[];return un(o,function(n,t,r){i(n,t,r)&&e.push(n)}),e}function I(s,t,e,n,r){var o=-1,a=s.length;for(e||(e=ht),r||(r=[]);++o<a;){var l=s[o];0<t&&e(l)?1<t?I(l,t-1,e,n,r):Cn(r,l):n||(r[r.length]=l)}return r}function U(n,t){return n&&fn(n,t,Tt)}function L(o,t){t=Q(t,o);for(var e=0,n=t.length;null!=o&&e<n;)o=o[jt(t[e++])];return e&&e==n?o:Kt}function P(o,t,e){return t=t(o),vn(o)?t:Cn(t,e(o))}function N(r){if(null==r)r=r===Kt?"[object Undefined]":"[object Null]";else if(Re&&Re in Object(r)){var t=Fe.call(r,Re),e=r[Re];try{r[Re]=Kt}catch(e){}var n=Ee.call(r);t?r[Re]=e:delete r[Re],r=n}else r=Ee.call(r);return r}function V(e){return Et(e)&&"[object Arguments]"==N(e)}function C(d,t,e,g,y){if(d===t)t=!0;else if(null==d||null==t||!Et(d)&&!Et(t))t=d!==d&&t!==t;else t:{var o=vn(d),u=vn(t),c=o?"[object Array]":bn(d),i=u?"[object Array]":bn(t),c="[object Arguments]"==c?"[object Object]":c,i="[object Arguments]"==i?"[object Object]":i,a="[object Object]"==c,u="[object Object]"==i;if((i=c==i)&&An(d)){if(!An(t)){t=!1;break t}o=!0,a=!1}if(i&&!a)y||(y=new S),t=o||Sn(d)?ot(d,t,e,g,C,y):rt(d,t,c,e,g,C,y);else{if(!(1&e)&&(o=a&&Fe.call(d,"__wrapped__"),c=u&&Fe.call(t,"__wrapped__"),o||c)){d=o?d.value():d,t=c?t.value():t,y||(y=new S),t=C(d,t,e,g,y);break t}if(i){e:if(y||(y=new S),o=1&e,c=it(d),u=c.length,i=it(t).length,u==i||o){for(a=u;a--;){var _=c[a];if(o?!(_ in t):!Fe.call(t,_)){t=!1;break e}}if((i=y.get(d))&&y.get(t))t=i==t;else{i=!0,y.set(d,t),y.set(t,d);for(var l=o;++a<u;){var _=c[a],m=d[_],b=t[_];if(g)var h=o?g(b,m,_,t,d,y):g(m,b,_,d,t,y);if(h===Kt?m!==b&&!C(m,b,e,g,y):!h){i=!1;break}l||(l="constructor"==_)}i&&!l&&(e=d.constructor,g=t.constructor,e!=g&&"constructor"in d&&"constructor"in t&&!("function"==typeof e&&e instanceof e&&"function"==typeof g&&g instanceof g)&&(i=!1)),y.delete(d),y.delete(t),t=i}}else t=!1;}else t=!1}}return t}function B(s,t){var e=t.length,n=e;if(null==s)return!n;for(s=Object(s);e--;){var a=t[e];if(a[2]?a[1]!==s[a[0]]:!(a[0]in s))return!1}for(;++e<n;){var a=t[e],o=a[0],l=s[o],d=a[1];if(a[2]){if(l===Kt&&!(o in s))return!1;}else if(a=new S,void 0===Kt?!C(d,l,3,void 0,a):1)return!1}return!0}function T(e){return"function"==typeof e?e:null==e?Wt:"object"==Tn(e)?vn(e)?D(e[0],e[1]):R(e):Jt(e)}function M(o,i){var e=-1,n=At(o)?Array(o.length):[];return un(o,function(r,t,o){n[++e]=i(r,t,o)}),n}function R(o){var t=ct(o);return 1==t.length&&t[0][2]?mt(t[0][0],t[0][1]):function(e){return e===o||B(e,t)}}function D(o,t){return dt(o)&&t===t&&!Ot(t)?mt(jt(o),t):function(e){var n=Ct(e,o);return n===Kt&&n===t?Bt(e,o):C(t,n,3)}}function $(o,i,d){var e=-1;return i=c(i.length?i:[Wt],l(st())),o=M(o,function(n){return{a:c(i,function(t){return t(n)}),b:++e,c:n}}),a(o,function(n,t){var e;t:{e=-1;for(var r=n.a,o=t.a,u=r.length,c=d.length;++e<u;){var i;e:{i=r[e];var a=o[e];if(i!==a){var g=i!==Kt,f=null===i,_=i===i,m=Ut(i),k=a!==Kt,x=null===a,S=a===a,A=Ut(a);if(!x&&!A&&!m&&i>a||m&&k&&S&&!x&&!A||f&&k&&S||!g&&S||!_){i=1;break e}if(!f&&!m&&!A&&i<a||A&&g&&_&&!f&&!m||x&&g&&_||!k&&_||!S){i=-1;break e}}i=0}if(i){e=e>=c?i:i*("desc"==d[e]?-1:1);break t}}e=n.b-t.b}return e})}function J(n){return function(t){return L(t,n)}}function q(e){return gn(bt(e,Wt),e+"")}function H(o,i){var e;return un(o,function(s,t,r){return e=i(s,t,r),!e}),!!e}function K(n){if("string"==typeof n)return n;if(vn(n))return c(n,K)+"";if(Ut(n))return ln?ln.call(n):"";var o=n+"";return"0"==o&&1/n==-Gt?"-0":o}function G(e){return Ft(e)?e:[]}function Q(n,t){return vn(n)?n:dt(n,t)?[n]:_n(Vt(n))}function X(o,t){if(t)return o.slice();var e=o.length,e=De?De(e):new o.constructor(e);return o.copy(e),e}function Y(n){var t=new n.constructor(n.byteLength);return new Ue(t).set(new Ue(n)),t}function Z(o,t){var e=-1,n=o.length;for(t||(t=Array(n));++e<n;)t[e]=o[e];return t}function et(s,t,e){var a=!e;e||(e={});for(var r=-1,o=t.length;++r<o;){var l=t[r],d=Kt;d===Kt&&(d=s[l]),a?w(e,l,d):v(e,l,d)}return e}function tt(n,t){return et(n,hn(n),t)}function nt(n,t){return et(n,yn(n),t)}function ot(l,t,d,n,r,o){var e=1&d,c=l.length,u=t.length;if(c!=u&&!(e&&u>c))return!1;if((u=o.get(l))&&o.get(t))return u==t;var u=-1,a=!0,g=2&d?new j:Kt;for(o.set(l,t),o.set(t,l);++u<c;){var s=l[u],f=t[u];if(n)var h=e?n(f,s,u,t,l,o):n(s,f,u,l,t,o);if(h!==Kt){if(h)continue;a=!1;break}if(g){if(!i(t,function(i,t){if(!g.has(t)&&(s===i||r(s,i,d,n,o)))return g.push(t)})){a=!1;break}}else if(s!==f&&!r(s,f,d,n,o)){a=!1;break}}return o.delete(l),o.delete(t),a}function rt(s,a,l,n,r,o,d){switch(l){case"[object DataView]":if(s.byteLength!=a.byteLength||s.byteOffset!=a.byteOffset)break;s=s.buffer,a=a.buffer;case"[object ArrayBuffer]":if(s.byteLength!=a.byteLength||!o(new Ue(s),new Ue(a)))break;return!0;case"[object Boolean]":case"[object Date]":case"[object Number]":return xt(+s,+a);case"[object Error]":return s.name==a.name&&s.message==a.message;case"[object RegExp]":case"[object String]":return s==a+"";case"[object Map]":var c=f;case"[object Set]":if(c||(c=g),s.size!=a.size&&!(1&n))break;return(l=d.get(s))?l==a:(n|=2,d.set(s,a),a=ot(c(s),c(a),n,r,o,d),d.delete(s),a);case"[object Symbol]":if(cn)return cn.call(s)==cn.call(a);}return!1}function it(e){return P(e,Tt,hn)}function at(e){return P(e,Mt,yn)}function st(){var e=d.iteratee||$t,e=e===$t?T:e;return arguments.length?e(arguments[0],arguments[1]):e}function lt(o,t){var i=o.__data__,n=Tn(t);return("string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==t:null===t)?i["string"==typeof t?"string":"hash"]:i.map}function ct(i){for(var t=Tt(i),e=t.length;e--;){var n=t[e],r=i[n];t[e]=[n,r,r===r&&!Ot(r)]}return t}function pt(o,t){var e=null==o?Kt:o[t];return(!Ot(e)||Ie&&Ie in e?0:(wt(e)?$e:le).test(St(e)))?e:Kt}function ut(o){var t=o.length,e=o.constructor(t);return t&&"string"==typeof o[0]&&Fe.call(o,"index")&&(e.index=o.index,e.input=o.input),e}function ft(i,s,r,o){var a=i.constructor;return"[object ArrayBuffer]"===s?Y(i):"[object Boolean]"===s||"[object Date]"===s?new a(+i):"[object DataView]"===s?(s=o?Y(i.buffer):i.buffer,new i.constructor(s,i.byteOffset,i.byteLength)):"[object Float32Array]"===s||"[object Float64Array]"===s||"[object Int8Array]"===s||"[object Int16Array]"===s||"[object Int32Array]"===s||"[object Uint8Array]"===s||"[object Uint8ClampedArray]"===s||"[object Uint16Array]"===s||"[object Uint32Array]"===s?(s=o?Y(i.buffer):i.buffer,new i.constructor(s,i.byteOffset,i.length)):"[object Map]"===s?(s=o?r(f(i),1):f(i),e(s,In,new i.constructor)):"[object Number]"===s||"[object String]"===s?new a(i):"[object RegExp]"===s?(s=new i.constructor(i.source,ie.exec(i)),s.lastIndex=i.lastIndex,s):"[object Set]"===s?(s=o?r(g(i),1):g(i),e(s,t,new i.constructor)):"[object Symbol]"===s?cn?Object(cn.call(i)):{}:void 0}function ht(e){return vn(e)||mn(e)||!!(Ce&&e&&e[Ce])}function gt(n,o){return o=null==o?9007199254740991:o,!!o&&("number"==typeof n||be.test(n))&&-1<n&&0==n%1&&n<o}function yt(o,t,e){if(!Ot(e))return!1;var n=Tn(t);return!("number"==n?!(At(e)&&gt(t,e.length)):!("string"==n&&t in e))&&xt(e[t],o)}function dt(o,t){if(vn(o))return!1;var e=Tn(o);return"number"==e||"symbol"==e||"boolean"==e||null==o||Ut(o)||ne.test(o)||!ee.test(o)||null!=t&&o in Object(t)}function _t(n){var t=n&&n.constructor;return n===("function"==typeof t&&t.prototype||ke)}function mt(o,t){return function(e){return null!=e&&e[o]===t&&(t!==Kt||o in Object(e))}}function bt(n,t){var e,e=Ke(e===Kt?n.length-1:e,0);return function(){for(var r=arguments,o=-1,s=Ke(r.length-e,0),a=Array(s);++o<s;)a[o]=r[e+o];for(o=-1,s=Array(e+1);++o<e;)s[o]=r[o];return s[e]=t(a),Pn(n,this,s)}}function jt(n){if("string"==typeof n||Ut(n))return n;var o=n+"";return"0"==o&&1/n==-Gt?"-0":o}function St(e){if(null!=e){try{return xe.call(e)}catch(e){}return e+""}return""}function kt(n,t){return(vn(n)?Nn:un)(n,st(t,3))}function vt(i,s){function a(){var e=arguments,t=s?s.apply(this,e):e[0],n=a.cache;return n.has(t)?n.get(t):(e=i.apply(this,e),a.cache=n.set(t,e)||n,e)}if("function"!=typeof i||null!=s&&"function"!=typeof s)throw new TypeError("Expected a function");return a.cache=new(vt.Cache||b),a}function xt(n,t){return n===t||n!==n&&t!==t}function At(e){return null!=e&&zt(e.length)&&!wt(e)}function Ft(e){return Et(e)&&At(e)}function wt(e){return!!Ot(e)&&(e=N(e),"[object Function]"==e||"[object GeneratorFunction]"==e||"[object AsyncFunction]"==e||"[object Proxy]"==e)}function zt(e){return"number"==typeof e&&-1<e&&0==e%1&&9007199254740991>=e}function Ot(n){var t=Tn(n);return null!=n&&("object"==t||"function"==t)}function Et(e){return null!=e&&"object"==Tn(e)}function It(e){return"string"==typeof e||!vn(e)&&Et(e)&&"[object String]"==N(e)}function Ut(e){return"symbol"==Tn(e)||Et(e)&&"[object Symbol]"==N(e)}function Lt(e){return e?(e=Nt(e),e===Gt||e===-Gt?1.7976931348623157e308*(0>e?-1:1):e===e?e:0):0===e?e:0}function Pt(n){n=Lt(n);var t=n%1;return n===n?t?n-t:n:0}function Nt(n){if("number"==typeof n)return n;if(Ut(n))return Qt;if(Ot(n)&&(n="function"==typeof n.valueOf?n.valueOf():n,n=Ot(n)?n+"":n),"string"!=typeof n)return 0===n?n:+n;n=n.replace(ue,"");var o=fe.test(n);return o||se.test(n)?je(n.slice(2),o?2:8):ae.test(n)?Qt:+n}function Vt(e){return null==e?"":K(e)}function Ct(o,t,e){return o=null==o?Kt:L(o,t),o===Kt?e:o}function Bt(s,t){var e;if(e=null!=s){e=s;var n=Q(t,e);for(var r,i=-1,o=n.length,a=!1;++i<o&&(r=jt(n[i]),!!(a=null!=e&&null!=e&&r in Object(e)));)e=e[r];a||++i!=o?e=a:(o=null==e?0:e.length,e=!!o&&zt(o)&&gt(r,o)&&(vn(e)||mn(e)))}return e}function Tt(o){if(At(o))o=k(o);else if(_t(o)){var t,e=[];for(t in Object(o))Fe.call(o,t)&&"constructor"!=t&&e.push(t);o=e}else o=qe(o);return o}function Mt(o){if(At(o))o=k(o,!0);else if(Ot(o)){var t,e=_t(o),i=[];for(t in o)("constructor"!=t||!e&&Fe.call(o,t))&&i.push(t);o=i}else{if(t=[],null!=o)for(e in Object(o))t.push(e);o=t}return o}function Rt(e){return null==e?[]:p(e,Tt(e))}function Dt(e){return function(){return e}}function Wt(e){return e}function $t(e){return T("function"==typeof e?e:z(e,1))}function Jt(e){return dt(e)?r(jt(e)):J(e)}function qt(){return[]}function Ht(){return!1}var Kt,Gt=1/0,Qt=NaN,ee=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,ne=/^\w*$/,re=/^\./,oe=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,ue=/^\s+|\s+$/g,ce=/\\(\\)?/g,ie=/\w*$/,ae=/^[-+]0x[0-9a-f]+$/i,fe=/^0b[01]+$/i,le=/^\[object .+?Constructor\]$/,se=/^0o[0-7]+$/i,be=/^(?:0|[1-9]\d*)$/,he={};he["[object Float32Array]"]=he["[object Float64Array]"]=he["[object Int8Array]"]=he["[object Int16Array]"]=he["[object Int32Array]"]=he["[object Uint8Array]"]=he["[object Uint8ClampedArray]"]=he["[object Uint16Array]"]=he["[object Uint32Array]"]=!0,he["[object Arguments]"]=he["[object Array]"]=he["[object ArrayBuffer]"]=he["[object Boolean]"]=he["[object DataView]"]=he["[object Date]"]=he["[object Error]"]=he["[object Function]"]=he["[object Map]"]=he["[object Number]"]=he["[object Object]"]=he["[object RegExp]"]=he["[object Set]"]=he["[object String]"]=he["[object WeakMap]"]=!1;var pe={};pe["[object Arguments]"]=pe["[object Array]"]=pe["[object ArrayBuffer]"]=pe["[object DataView]"]=pe["[object Boolean]"]=pe["[object Date]"]=pe["[object Float32Array]"]=pe["[object Float64Array]"]=pe["[object Int8Array]"]=pe["[object Int16Array]"]=pe["[object Int32Array]"]=pe["[object Map]"]=pe["[object Number]"]=pe["[object Object]"]=pe["[object RegExp]"]=pe["[object Set]"]=pe["[object String]"]=pe["[object Symbol]"]=pe["[object Uint8Array]"]=pe["[object Uint8ClampedArray]"]=pe["[object Uint16Array]"]=pe["[object Uint32Array]"]=!0,pe["[object Error]"]=pe["[object Function]"]=pe["[object WeakMap]"]=!1;var ye,je=parseInt,ve="object"==("undefined"==typeof En?"undefined":Tn(En))&&En&&En.Object===Object&&En,ge="object"==("undefined"==typeof self?"undefined":Tn(self))&&self&&self.Object===Object&&self,_e=ve||ge||Function("return this")(),de="object"==("undefined"==typeof zn?"undefined":Tn(zn))&&zn&&!zn.nodeType&&zn,Ae=de&&"object"==("undefined"==typeof Fn?"undefined":Tn(Fn))&&Fn&&!Fn.nodeType&&Fn,me=Ae&&Ae.exports===de,we=me&&ve.process;t:{try{ye=we&&we.binding&&we.binding("util");break t}catch(e){}ye=void 0}var Oe=ye&&ye.isTypedArray,Se=Array.prototype,ke=Object.prototype,ze=_e["__core-js_shared__"],xe=Function.prototype.toString,Fe=ke.hasOwnProperty,Ie=function(){var e=/[^.]+$/.exec(ze&&ze.keys&&ze.keys.IE_PROTO||"");return e?"Symbol(src)_1."+e:""}(),Ee=ke.toString,$e=RegExp("^"+xe.call(Fe).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$"),Be=me?_e.Buffer:Kt,Me=_e.Symbol,Ue=_e.Uint8Array,De=Be?Be.f:Kt,Le=h(Object.getPrototypeOf),Pe=Object.create,Ne=ke.propertyIsEnumerable,Ve=Se.splice,Ce=Me?Me.isConcatSpreadable:Kt,Re=Me?Me.toStringTag:Kt,Te=function(){try{var e=pt(Object,"defineProperty");return e({},"",{}),e}catch(e){}}(),We=Object.getOwnPropertySymbols,Ge=Be?Be.isBuffer:Kt,qe=h(Object.keys),Ke=xn,He=Math.min,Je=Date.now,Qe=pt(_e,"DataView"),Xe=pt(_e,"Map"),Xt=pt(_e,"Promise"),Ze=pt(_e,"Set"),Yt=pt(_e,"WeakMap"),Zt=pt(Object,"create"),en=St(Qe),tn=St(Xe),nn=St(Xt),on=St(Ze),rn=St(Yt),an=Me?Me.prototype:Kt,cn=an?an.valueOf:Kt,ln=an?an.toString:Kt,sn=function(){function n(){}return function(t){return Ot(t)?Pe?Pe(t):(n.prototype=t,t=new n,n.prototype=Kt,t):{}}}();_.prototype.clear=function(){this.__data__=Zt?Zt(null):{},this.size=0},_.prototype.delete=function(e){return e=this.has(e)&&delete this.__data__[e],this.size-=e?1:0,e},_.prototype.get=function(n){var t=this.__data__;return Zt?(n=t[n],"__lodash_hash_undefined__"===n?Kt:n):Fe.call(t,n)?t[n]:Kt},_.prototype.has=function(n){var t=this.__data__;return Zt?t[n]!==Kt:Fe.call(t,n)},_.prototype.set=function(o,t){var e=this.__data__;return this.size+=this.has(o)?0:1,e[o]=Zt&&t===Kt?"__lodash_hash_undefined__":t,this},m.prototype.clear=function(){this.__data__=[],this.size=0},m.prototype.delete=function(n){var t=this.__data__;return n=x(t,n),!(0>n)&&(n==t.length-1?t.pop():Ve.call(t,n,1),--this.size,!0)},m.prototype.get=function(n){var t=this.__data__;return n=x(t,n),0>n?Kt:t[n][1]},m.prototype.has=function(e){return-1<x(this.__data__,e)},m.prototype.set=function(o,t){var e=this.__data__,n=x(e,o);return 0>n?(++this.size,e.push([o,t])):e[n][1]=t,this},b.prototype.clear=function(){this.size=0,this.__data__={hash:new _,map:new(Xe||m),string:new _}},b.prototype.delete=function(e){return e=lt(this,e).delete(e),this.size-=e?1:0,e},b.prototype.get=function(e){return lt(this,e).get(e)},b.prototype.has=function(e){return lt(this,e).has(e)},b.prototype.set=function(o,t){var e=lt(this,o),n=e.size;return e.set(o,t),this.size+=e.size==n?0:1,this},j.prototype.add=j.prototype.push=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this},j.prototype.has=function(e){return this.__data__.has(e)},S.prototype.clear=function(){this.__data__=new m,this.size=0},S.prototype.delete=function(n){var t=this.__data__;return n=t.delete(n),this.size=t.size,n},S.prototype.get=function(e){return this.__data__.get(e)},S.prototype.has=function(e){return this.__data__.has(e)},S.prototype.set=function(o,t){var e=this.__data__;if(e instanceof m){var n=e.__data__;if(!Xe||199>n.length)return n.push([o,t]),this.size=++e.size,this;e=this.__data__=new b(n)}return e.set(o,t),this.size=e.size,this};var un=function(i,t){return function(e,n){if(null==e)return e;if(!At(e))return i(e,n);for(var r=e.length,o=t?r:-1,s=Object(e);(t?o--:++o<r)&&!1!==n(s[o],o,s););return e}}(U),fn=function(e){return function(t,s,n){var r=-1,o=Object(t);n=n(t);for(var a,i=n.length;i--&&(a=n[e?i:++r],!1!==s(o[a],a,o)););return t}}(),pn=Te?function(n,t){return Te(n,"toString",{configurable:!0,enumerable:!1,value:Dt(t),writable:!0})}:Wt,hn=We?function(o){return null==o?[]:(o=Object(o),n(We(o),function(t){return Ne.call(o,t)}))}:qt,yn=We?function(n){for(var t=[];n;)Cn(t,hn(n)),n=Le(n);return t}:qt,bn=N;(Qe&&"[object DataView]"!=bn(new Qe(new ArrayBuffer(1)))||Xe&&"[object Map]"!=bn(new Xe)||Xt&&"[object Promise]"!=bn(Xt.resolve())||Ze&&"[object Set]"!=bn(new Ze)||Yt&&"[object WeakMap]"!=bn(new Yt))&&(bn=function(n){var t=N(n);if(n=(n="[object Object]"==t?n.constructor:Kt)?St(n):"")switch(n){case en:return"[object DataView]";case tn:return"[object Map]";case nn:return"[object Promise]";case on:return"[object Set]";case rn:return"[object WeakMap]";}return t});var gn=function(i){var t=0,e=0;return function(){var n=Je(),r=16-(n-e);if(!(e=n,0<r))t=0;else if(800<=++t)return arguments[0];return i.apply(Kt,arguments)}}(pn),_n=function(n){n=vt(n,function(e){return 500===o.size&&o.clear(),e});var o=n.cache;return n}(function(n){var i=[];return re.test(n)&&i.push(""),n.replace(oe,function(e,t,n,r){i.push(n?r.replace(ce,"$1"):t||e)}),i}),dn=q(function(i){var t=c(i,G);if(t.length&&t[0]===i[0]){i=t[0].length;for(var e=t.length,n=e,r=Array(e),o=1/0,d=[];n--;){var a=t[n],o=He(a.length,o);r[n]=120<=i&&120<=a.length?new j(n&&a):Kt}var a=t[0],g=-1,l=r[0];t:for(;++g<i&&d.length<o;){var s=a[g],f=s,s=0===s?0:s;if(l?!l.has(f):!u(d,f)){for(n=e;--n;){var h=r[n];if(h?!h.has(f):!u(t[n],f))continue t}l&&l.push(f),d.push(s)}}t=d}else t=[];return t}),jn=q(function(o,t){if(null==o)return[];var e=t.length;return 1<e&&yt(o,t[0],t[1])?t=[]:2<e&&yt(t[0],t[1],t[2])&&(t=[t[0]]),$(o,I(t,1),[])});vt.Cache=b;var mn=V(function(){return arguments}())?V:function(e){return Et(e)&&Fe.call(e,"callee")&&!Ne.call(e,"callee")},vn=Array.isArray,An=Ge||Ht,Sn=Oe?l(Oe):function(e){return Et(e)&&zt(e.length)&&!!he[N(e)]};d.constant=Dt,d.filter=function(o,t){return(vn(o)?n:E)(o,st(t,3))},d.flatten=function(e){return(null==e?0:e.length)?I(e,1):[]},d.intersection=dn,d.iteratee=$t,d.keys=Tt,d.keysIn=Mt,d.map=function(n,t){return(vn(n)?c:M)(n,st(t,3))},d.mapKeys=function(o,i){var e={};return i=st(i,3),U(o,function(n,t,r){w(e,i(n,t,r),n)}),e},d.mapValues=function(o,i){var e={};return i=st(i,3),U(o,function(n,t,r){w(e,t,i(n,t,r))}),e},d.memoize=vt,d.orderBy=function(o,t,e,n){return null==o?[]:(vn(t)||(t=null==t?[]:[t]),e=n?Kt:e,vn(e)||(e=null==e?[]:[e]),$(o,t,e))},d.property=Jt,d.sortBy=jn,d.transform=function(r,i,e){var t=vn(r),n=t||An(r)||Sn(r);if(i=st(i,4),null==e){var o=r&&r.constructor;e=n?t?new o:[]:Ot(r)&&wt(o)?sn(Le(r)):{}}return(n?Nn:U)(r,function(n,t,r){return i(e,n,t,r)}),e},d.values=Rt,d.clone=function(e){return z(e,4)},d.eq=xt,d.every=function(o,t,e){var n=vn(o)?Vn:O;return e&&yt(o,t,e)&&(t=Kt),n(o,st(t,3))},d.forEach=kt,d.get=Ct,d.hasIn=Bt,d.identity=Wt,d.includes=function(i,t,e,n){return i=At(i)?i:Rt(i),e=e&&!n?Pt(e):0,n=i.length,0>e&&(e=Ke(n+e,0)),It(i)?e<=n&&-1<i.indexOf(t,e):!!n&&-1<(t===t?y(i,t,e):o(i,s,e))},d.isArguments=mn,d.isArray=vn,d.isArrayLike=At,d.isArrayLikeObject=Ft,d.isBuffer=An,d.isFunction=wt,d.isLength=zt,d.isObject=Ot,d.isObjectLike=Et,d.isString=It,d.isSymbol=Ut,d.isTypedArray=Sn,d.stubArray=qt,d.stubFalse=Ht,d.some=function(o,t,e){var n=vn(o)?i:H;return e&&yt(o,t,e)&&(t=Kt),n(o,st(t,3))},d.toFinite=Lt,d.toInteger=Pt,d.toNumber=Nt,d.toString=Vt,d.each=kt,d.VERSION="4.17.4","function"==typeof W&&"object"==Tn(W.amd)&&W.amd?(_e._=d,W(function(){return d})):Ae?((Ae.exports=d)._=d,de._=d):_e._=d}).call(void 0)}).call(this,"undefined"==typeof global?"undefined"==typeof self?"undefined"==typeof window?{}:window:self:global)},{}]},{},[1])(1)});
  (function() {
  var pubElems = document.querySelectorAll(".publication");
  var yearElems = document.querySelectorAll(".year");

  var clearElem = document.getElementById("clear-filters");
  var highlightElem = document.getElementById("highlight");

  var data = [];
  var allYears = {};

  pubElems.forEach(function(element) {
    var item = JSON.parse(element.getAttribute("data-pub"));

    if (item.highlight) {
      item.highlight = "Yes";
    }

    allYears[item.year] = 1;

    item.element = element;

    data.push(item);
  });

  var engine = itemsjs(data, {
    aggregations: {
      venue_tags: {
        size: 5
      },
      authors: {
        size: 6
      },
      awards: {
        size: 5
      },
      highlight: {
        size: 1
      },
      tags: {
        size: 6
      },
      type: {
        size: 5
      }
    }
  });

  var query = { filters: {} };

  function setAggs(aggs) {
    document.querySelectorAll("#facets > .facet").forEach(function(facet) {
      var id = facet.getAttribute("id");

      var buckets = aggs[id].buckets;

      var el = facet.querySelector("ul");
      if (buckets.length === 0) {
        el.innerHTML = "Empty";
      } else {
        el.innerHTML = "";

        buckets.forEach(function(bucket) {
          if (query.filters[id] && query.filters[id].indexOf(bucket.key) >= 0) {
            bucket.in_query = true;
          }
        });

        var maxDocCount = Math.max.apply(
          null,
          buckets.map(function(bucket) {
            return bucket.doc_count;
          })
        );

        buckets.forEach(function(bucket) {
          var child = document.createElement("li");
          child.classList.add("mb2", "pointer");

          var wrap = document.createElement("span");
          child.appendChild(wrap);

          var text = document.createElement("span");
          text.innerText = bucket.key;
          text.setAttribute("title", bucket.key);
          var number = document.createElement("span");
          number.classList.add("gray", "f6");
          number.innerText = " (" + bucket.doc_count + ")";
          wrap.appendChild(text);
          wrap.appendChild(number);

          var barFull = document.createElement("div");
          barFull.classList.add("w-100", "bb", "b--black-20", "bw1", "mt1");
          child.append(barFull);

          var bar = document.createElement("div");
          bar.classList.add("bb", "b--cmu-red", "bw1");
          bar.style.marginBottom = "-.125rem";
          bar.style.width = "" + (bucket.doc_count / maxDocCount) * 100 + "%";
          barFull.append(bar);

          if (bucket.in_query) {
            child.classList.add("b");

            // remove filter
            child.onclick = function() {
              query.filters[id].splice(
                query.filters[id].indexOf(bucket.key),
                1
              );
              if (query.filters[id].length === 0) {
                delete query.filters[id];
              }
              search(query);
            };
          } else {
            // add to filter
            child.onclick = function() {
              if (query.filters[id]) {
                query.filters[id].push(bucket.key);
              } else {
                query.filters[id] = [bucket.key];
              }
              search(query);
            };
          }

          el.appendChild(child);
        });
      }
    });
  }

  // full text search is broken
  // var ftSearch = document.getElementById("ft-search");
  // ftSearch.oninput = function() {
  //   var val = ftSearch.value;

  //   if (val) {
  //     query.query = val;
  //   } else {
  //     delete query.val;
  //   }

  //   console.log(query);
  //   search(query);
  // }

  function search(query) {
    console.time("Search");

    var result = engine.search(Object.assign({ per_page: 1000 }, query));

    setAggs(result.data.aggregations);

    var counter = pubElems.length - result.data.items.length;

    document.getElementById("count_hidden").innerText = counter;
    document.getElementById("count_total").innerText = pubElems.length;

    pubElems.forEach(function(element) {
      element.classList.add("dn");
    });

    var visibleYears = {};
    result.data.items.forEach(function(item) {
      item.element.classList.remove("dn");
      visibleYears[item.year] = 1;
    });

    yearElems.forEach(function(element) {
      element.classList.add("dn");
    });
    Object.keys(allYears).forEach(function(year) {
      if (year in visibleYears) {
        document.getElementById("y" + year).classList.remove("dn");
      }
    });

    // show or hide notification about filtered papers
    if (Object.keys(query.filters).length || query.query) {
      clearElem.classList.remove("dn");
    } else {
      clearElem.classList.add("dn");
    }

    highlightElem.checked = !!query.filters.highlight;

    console.timeEnd("Search");
  }

  highlightElem.onchange = function() {
    if (highlightElem.checked) {
      query.filters.highlight = ["Yes"];
    } else {
      delete query.filters.highlight;
    }
    search(query);
  };

  clearElem.onclick = function() {
    query = { filters: {} };
    search(query);
  };

  search(query);

  document.getElementById("facets").classList.remove("dn");
  document.getElementById("only-highlight").classList.remove("dn");
})();

</script>

<script>
  //From https://codepen.io/Anotherdago/pen/yjVxOB
  //Add basic styles for active tabs
    $('.accordion__item-label').on('click', function() {
      $(this).addClass('blue');
      $(this).parent('.accordion__item').siblings().find('.accordion__item-label').removeClass('blue');
    });
</script>

  </div>
</section>

      </main>
    </div>

    <footer class="bt white mt5 flex-shrink-0">
      <div class="w-100 mw8 ph4-l ph3 center pv3 footerinfo">
        <ul class="list pl0">
          <li>
            <a href="/index" class="link white dib underline-dot-white hover-cmu-red mv1">Augmented Perception Lab</a>, 
            <a href="https://hcii.cmu.edu" class="link white underline-dot-white hover-cmu-red mv1">Human-Computer Interaction Institute</a>,
            <a href="https://www.cs.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">School of Computer Science</a>,
            <a href="https://www.cmu.edu" class="link white dib underline-dot-white hover-cmu-red mv1">Carnegie Mellon University</a>
            <!-- <span ></span>  -->
          </li>
          <li>
            <span>
            Newell Simon Hall,  5000 Forbes Ave, Pittsburgh, PA 15213, United States, <a href="/contact" class="link white dim underline-dot-white hover-cmu-red">How to find us.</a>
            </span>
          </li>
          <li class="pv1">
            <abbr title="Last build on 2021-04-01" class="white" style="list-style: height 2em; text-decoration: none;">Last update April 2021</abbr>              
          </li>
        </ul>
      </div>
    </footer>
  </body>
</html>
