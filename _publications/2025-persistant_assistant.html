---
layout: publication
year: 2025
month: 05
selected: false
coming-soon: false
hidden: false
external : false
# link: https://dl.acm.org/doi/10.1145/3613904.3642394
pdf: https://dl.acm.org/doi/10.1145/3706598.3714317
title: "Persistent Assistant: Seamless Everyday AI Interactions via Intent Grounding and Multimodal Feedback"
authors:
  - Hyunsung Cho
  - Jacqui Fashimpaur
  - Naveen Sendhilnathan
  - Jonathan Browder
  - David Lindlbauer
  - Tanya R. Jonker
  - Kashyap Todi
# blog:
# doi: 10.1145/3613904.3642394
venue_location: Yokohama, Japan
venue_url: https://chi2025.acm.org/
venue_tags:
  - ACM CHI
type:
  - Conference
tags:
  - Science
  - Extended Reality
  - Adaptive User Interfaces
venue: ACM CHI

#video-thumb: 7K3eouLCcSw
#video-30sec: 7K3eouLCcSw
video-suppl: eOUbB9NwqZw
#video-talk-5min: l9ycUrf50TE
#video-thumb: KufsjCVCMgQ

bibtex: "@inproceedings {Cho25PersistentAssistant, \n
author = {Cho, Hyunsung and Fashimpaur, Jacqui and Sendhilnathan, Naveen and Browder, Jonathan and Lindlbauer, David and Jonker, Tanya R. and Todi, Kashyap}, \n
title = {Persistent Assistant: Seamless Everyday AI Interactions via Intent Grounding and Multimodal Feedback}, \n
year = {2025}, \n
publisher = {Association for Computing Machinery}, \n
address = {New York, NY, USA}, \n
keywords = {Wearable AI assistants, grounding, multimodal interaction, gaze and gesture input, haptic and speech feedback}, \n
location = {Yokohama, Japan}, \n
series = {CHI '25} \n
}"

---

Current AI assistants predominantly use natural language interactions, which can be time-consuming and cognitively demanding, especially for frequent, repetitive tasks in daily life. We propose Persistent Assistant, a framework for seamless and unobtrusive interactions with AI assistants. The framework has three key functionalities: (1) efficient intent specification through grounded interactions, (2) seamless target referencing through embodied input, and (3) intuitive response comprehension through multimodal perceptible feedback. We developed a proof-of-concept system for everyday decision-making tasks, where users can easily repeat queries over multiple objects using eye gaze and pinch gesture, as well as receiving multimodal haptic and speech feedback. Our study shows that multimodal feedback enhances user experience and preference by reducing physical demand, increasing perceived speed, and enabling intuitive and instinctive human-AI assistant interaction. We discuss how our framework can be applied to build seamless and unobtrusive AI assistants for everyday persistent tasks.