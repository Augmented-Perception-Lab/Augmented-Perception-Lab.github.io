---
layout: publication
year: 2024
month: 10
selected: false
coming-soon: false
hidden: false
external : false
# link: https://dl.acm.org/doi/10.1145/3472749.3474750
pdf: https://doi.org/10.1145/3654777.3676384
title: "SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR"
authors:
  - Hyunsung Cho  
  - Naveen Sendhilnathan
  - Michael Nebeling
  - Tianyi Wang
  - Purnima Padmanabhan
  - Jonathan Browder
  - David Lindlbauer
  - Tanya R. Jonker
  - Kashyap Todi

# blog: https://interactive-structures.org/publications/2023-10-parametric-haptics/
doi: 10.1145/3654777.3676384
venue_location: Pittsburgh, PA, USA
venue_url: https://uist.acm.org/2024/
venue_tags:
  - ACM UIST
type:
  - Conference
tags:
  - Computational Interaction
  - Audio Augmented Reality
  - Extended Reality
  - Machine Learning
  - Haptics
venue: ACM UIST

#video-thumb: hiNdhJqOQ2E
#video-30sec: hiNdhJqOQ2E
#video-suppl: PIUCEdw4UqA
#video-talk-5min: l9ycUrf50TE
#video-talk-15min: l9ycUrf50TE

bibtex: "@inproceedings {Cho2024SonoHaptics, \n
author = {Cho, Hyunsung and Sendhilnathan, Naveen and Nebeling, Michael and Wang, Tianyi and Padmanabhan, Purnima and Browder, Jonathan and Lindlbauer, David and Jonker, Tanya R. and Todi, Kashyap}, \n
title = {SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR}, \n
year = {2024}, \n
publisher = {Association for Computing Machinery}, \n
address = {New York, NY, USA}, \n
doi = {10.1145/3654777.3676384}, \n
keywords = {Extended Reality, Sonification, Haptics, Multimodal Feedback, Computational Interaction, Gaze-based Selection}, \n
location = {Pittsburgh, PA, USA}, \n
series = {UIST '24} \n
}"

---

We introduce SonoHaptics, an audio-haptic cursor for gaze-based 3D object selection. SonoHaptics addresses challenges around providing accurate visual feedback during gaze-based selection in Extended Reality (XR), e.g., lack of world-locked displays in no- or limited-display smart glasses and visual inconsistencies. To enable users to distinguish objects without visual feedback, SonoHaptics employs the concept of cross-modal correspondence in human perception to map visual features of objects (color, size, position, material) to audio-haptic properties (pitch, amplitude, direction, timbre). We contribute data-driven models for determining cross-modal mappings of visual features to audio and haptic features, and a computational approach to automatically generate audio-haptic feedback for objects in the user's environment. SonoHaptics provides global feedback that is unique to each object in the scene, and local feedback to amplify differences between nearby objects. Our comparative evaluation shows that SonoHaptics enables accurate object identification and selection in a cluttered scene without visual feedback.